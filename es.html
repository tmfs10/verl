<!DOCTYPE html>
<!-- saved from url=(0033)https://arxiv.org/html/2509.24372 -->
<html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning</title>
<!--Generated on Mon Sep 29 07:08:57 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="./es_files/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css">
<script src="./es_files/bootstrap.bundle.min.js"></script>
<script src="./es_files/html2canvas.min.js"></script>
<script src="./es_files/addons_new.js"></script>
<script src="./es_files/feedbackOverlay.js"></script>
<!--<base href="/html/2509.24372v1/">--><base href="."><link rel="stylesheet" href="./es_files/utz6mli.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="./es_files/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2509.24372">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main &gt;.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="./es_files/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2509.24372v1/#myForm" onclick="event.preventDefault(); var modal = document.getElementById(&#39;myForm&#39;); modal.style.display = &#39;block&#39;; bugReportState.setInitiateWay(&#39;Header&#39;);">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2509.24372v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2509.24372v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2509.24372#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S1" title="In Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S2" title="In Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S3" title="In Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S3.SS1" title="In 3 Method ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Basic ES Algorithm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S3.SS2" title="In 3 Method ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4" title="In Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Empirical Studies</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS1" title="In 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Performance in the Countdown Task</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS1.SSS0.Px1" title="In 4.1 Performance in the Countdown Task ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">Countdown task.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS1.SSS0.Px2" title="In 4.1 Performance in the Countdown Task ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">Experimental Setup.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS1.SSS0.Px3" title="In 4.1 Performance in the Countdown Task ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">ES improves upon PPO and GRPO across all tested models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS1.SSS0.Px4" title="In 4.1 Performance in the Countdown Task ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">ES is more sample efficient than RL.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS1.SSS0.Px5" title="In 4.1 Performance in the Countdown Task ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">ES is effective on smaller models.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS2" title="In 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Behavioral Differences between ES and RL in fine-tuning for conciseness</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS2.SSS0.Px1" title="In 4.2 Behavioral Differences between ES and RL in fine-tuning for conciseness ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">Conciseness task.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS2.SSS0.Px2" title="In 4.2 Behavioral Differences between ES and RL in fine-tuning for conciseness ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">Behavior metrics.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS2.SSS0.Px3" title="In 4.2 Behavioral Differences between ES and RL in fine-tuning for conciseness ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">ES discovers a dominant Pareto front.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS2.SSS0.Px4" title="In 4.2 Behavioral Differences between ES and RL in fine-tuning for conciseness ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">ES is more robust against reward hacking.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.SS2.SSS0.Px5" title="In 4.2 Behavioral Differences between ES and RL in fine-tuning for conciseness ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">ES fine-tuning is reliable across runs.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S5" title="In Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S6" title="In Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1" title="In Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS1" title="In Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS1.SSS0.Px1" title="In A.1 Experimental Setup ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">Experimental setup for the Countdown experiments.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS1.SSS0.Px2" title="In A.1 Experimental Setup ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">Experimental setup for the Conciseness experiments.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS2" title="In Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Examples of Countdown Task Outputs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS3" title="In Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Extended Conciseness Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS3.SSS0.Px1" title="In A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title">GRPO with different learning rates.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS4" title="In Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Training Curves and Accuracy Improvement of ES and RL on the Countdown Task</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS5" title="In Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Parameter Magnitude Shifts by Evolutionary fine-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2509.24372#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0</a><div id="watermark-tr">arXiv:2509.24372v1 [cs.LG] 29 Sep 2025</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xin Qiu   
<br class="ltx_break">Cognizant AI Lab
&amp;Yulu Gan <span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
<br class="ltx_break">MIT 
<br class="ltx_break">&amp;Conor F. Hayes <span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
<br class="ltx_break">Cognizant AI Lab
&amp;Qiyao Liang <span class="ltx_note ltx_role_footnotemark" id="footnotex3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">3</span></span></span></span>
<br class="ltx_break">MIT
</span><span class="ltx_author_notes">Equal ContributionProject Lead, Correspondence to: Xin Qiu &lt;qiuxin.nju@gmail.com&gt;Work done during internship at Cognizant AI Lab</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elliot Meyerson 
<br class="ltx_break">Cognizant AI Lab
&amp;Babak Hodjat 
<br class="ltx_break">Cognizant AI Lab
&amp;Risto Miikkulainen 
<br class="ltx_break">Cognizant AI Lab, 
<br class="ltx_break">UT Austin
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p">Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: <a class="ltx_ref ltx_href" href="https://github.com/VsonicV/es-fine-tuning-paper" title="">https://github.com/VsonicV/es-fine-tuning-paper</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p">The rapid development of more capable large language models <cite class="ltx_cite ltx_citemacro_citep">(LLMs; Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib67" title="">2023</a>; Achiam et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib1" title="">2024</a>; AI@Meta, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib3" title="">2024</a>; Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib26" title="">2024</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib32" title="">2024</a>; Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib5" title="">2025</a>; Google, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib18" title="">2025</a>)</cite> has made many scientific and engineering domains amenable to AI-based automation <cite class="ltx_cite ltx_citemacro_citep">(Singhal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib58" title="">2023</a>; Wu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib73" title="">2023</a>; Rozière et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib47" title="">2024</a>; Romera-Paredes et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib46" title="">2024</a>)</cite>. As a result, fine-tuning the pre-trained models to accommodate specific tasks and to improve alignment with user preferences has become an important part of the LLM deployment pipeline <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib39" title="">2022</a>; Rafailov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib43" title="">2023</a>; Latif &amp; Zhai, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib29" title="">2024</a>; Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib19" title="">2025a</a>)</cite>. Reinforcement learning (RL) is currently the predominant choice for such fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib39" title="">2022</a>; Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib6" title="">2022</a>; Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib57" title="">2024</a>; Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib19" title="">2025a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib20" title="">b</a>; Srivastava &amp; Aggarwal, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib61" title="">2025</a>)</cite>. Several challenges have emerged: First, RL methods incur low sample efficiency <cite class="ltx_cite ltx_citemacro_citep">(Vemula et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib69" title="">2019</a>)</cite> and high variance of the gradient estimator <cite class="ltx_cite ltx_citemacro_citep">(Salimans et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>; Sutton &amp; Barto, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib65" title="">2018</a>)</cite> when handling long-horizon rewards, which is a common case for LLM fine-tuning with outcome-only rewards. Proper credit assignment at token level for RL fine-tuning methods is difficult <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib77" title="">2025</a>; Song et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib59" title="">2025</a>; Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib20" title="">2025b</a>)</cite> and possibly unhelpful <cite class="ltx_cite ltx_citemacro_citep">(Uesato et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib68" title="">2022</a>; Jia et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib25" title="">2025</a>; Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib20" title="">2025b</a>)</cite>. Second, RL techniques are sensitive to the choice of base LLMs, resulting in inconsistent fine-tuning performance across different models <cite class="ltx_cite ltx_citemacro_citep">(Gandhi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib15" title="">2025</a>)</cite>. Third, RL techniques have an inherent tendency to hack the reward function, leading to undesirable behaviors <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib16" title="">2023</a>; Denison et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib11" title="">2024</a>; Fu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib14" title="">2025</a>)</cite>. Fourth, RL fine-tuning is often unstable across multiple runs even with the same parametric setup, increasing the cost of fine-tuning significantly <cite class="ltx_cite ltx_citemacro_citep">(Choshen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib8" title="">2020</a>; Zhong et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib79" title="">2025</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p">Evolution Strategies (ES), a class of population-based zeroth-order optimization algorithms, is a possible alternative. ES has several unique advantages over RL in traditional control and gaming problems: it is highly parallel, tolerates long-horizon rewards well, explores extensively, needs less computation (no backpropagation), and is robust to setup parameters <cite class="ltx_cite ltx_citemacro_citep">(Salimans et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>; Chrabaszcz et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib9" title="">2018</a>; Conti et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib10" title="">2018</a>)</cite>. However, ES has received much less attention than RL during the LLM era. Standard ES works by searching and optimizing in the original parameter space directly, for which the dimension was no more than a few million in past implementations <cite class="ltx_cite ltx_citemacro_citep">(Salimans et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib76" title="">2017</a>; Lehman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib30" title="">2018</a>; Lorenc &amp; Neruda, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib34" title="">2025</a>)</cite>. It was assumed that if the model is very large, it is significantly more difficult and sample-inefficient to explore in parameter space compared to action-space <cite class="ltx_cite ltx_citemacro_citep">(Vemula et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib69" title="">2019</a>)</cite>. The number of parameters in LLMs is usually on the order of billions, which may seem infeasible for ES to directly tackle. Existing workarounds include applying ES only to the last layer of the original model <cite class="ltx_cite ltx_citemacro_citep">(Toledano-López et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib66" title="">2022</a>)</cite>, using ES to fine-tune a lower-dimensional adapter <cite class="ltx_cite ltx_citemacro_citep">(Jin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib27" title="">2024</a>)</cite>, and searching in action space as in standard RL <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib23" title="">2025</a>)</cite>. Directly searching in the full parameter space of LLMs has remained a daunting challenge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p">This paper is aimed at meeting this challenge. For the first time, ES is scaled to multi-billion-parameter search spaces, by searching directly over the full parameter space of LLMs in fine-tuning tasks. The approach is based on a memory-efficient implementation of an algorithmically simplified ES variant, with support for parallelization within and across GPUs. Performance is compared with state-of-the-art (SOTA) RL methods in fine-tuning various LLMs in a standard reasoning benchmark task, and behavioral differences from RL are analyzed in terms of fine-tuning for conciseness. This version of ES was able to search directly over billions of parameters, and exhibit surprisingly good fine-tuning performance compared to RL methods in multiple aspects:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p">ES only needs response-level rewards, making it a perfect fit for fine-tuning on reasoning tasks that have only sparse long-horizon outcome rewards. In particular, ES obtained significantly better fine-tuned models than RL in the Countdown task with such rewards</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p">Counterintuitively, even though ES explores in the parameter space with billions of parameters, it is more sample efficient than RL methods that explore in the action space, which is much smaller. Further, ES was able to find good solutions with a population size of only 30. As a comparison, previous ES implementations <cite class="ltx_cite ltx_citemacro_citep">(Salimans et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib76" title="">2017</a>; Lehman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib30" title="">2018</a>; Lorenc &amp; Neruda, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib34" title="">2025</a>)</cite> utilized a population size of 10,000 or more with much smaller models (i.e. millions of parameters or less).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p">ES is significantly more robust than RL across different LLMs. While RL fine-tuning failed on some LLMs, ES provided good fine-tuning for all of them. ES benefits from its exploration in parameter space, making it less sensitive to initial states of the LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p">Whereas RL tends to hack the reward function if no other penalty is added, ES consistently maintains reasonable behaviors during fine-tuning. The main reason is that ES optimizes a solution distribution <cite class="ltx_cite ltx_citemacro_citep">(Lehman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib30" title="">2018</a>)</cite>, which is more difficult to hack, while RL optimizes a single solution.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p">ES’s behavior is more consistent than RL’s across different runs. This property can significantly reduce expected cost of fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i6.p1">
<p class="ltx_p">Fine-tuning with ES is based on inference, and therefore no backpropagation calculations are needed. A significant amount of GPU memory can therefore be saved <cite class="ltx_cite ltx_citemacro_citep">(Malladi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib35" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p">The study reported in this paper serves as a first step in demonstrating the potential of ES for fine-tuning LLMs. The surprising and counterintuitive findings motivates further work scaling up ES to larger LLM fine-tuning tasks. Given the unique advantages over the state of the art, ES opens up new opportunities in parameter-space exploration, outcome-only fine-tuning, and large-scale distributed post-training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p">Evolution Strategies <cite class="ltx_cite ltx_citemacro_citep">(ES, Rechenberg, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib44" title="">1973</a>; Schwefel, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib55" title="">1977</a>)</cite> are a class of evolutionary algorithms (EAs) for solving numerical optimization problems. The main idea is to sample a population of solutions through perturbations, then recombine the perturbed solutions based on their fitness values to form the population for the next generation. This process repeats until a termination condition is triggered, e.g., the maximum number of generations is reached. Among the different variants of ES, CMA-ES <cite class="ltx_cite ltx_citemacro_citep">(Hansen &amp; Ostermeier, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib22" title="">2001</a>)</cite>, which utilizes a multivariate Gaussian distribution with full covariance matrix to sample the population, and natural ES <cite class="ltx_cite ltx_citemacro_citep">(Wierstra et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib71" title="">2008</a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib72" title="">2014</a>)</cite>, which uses natural gradient to guide the search, are two popular methods for traditional optimization problems. Although ES has long been used to evolve parameters of neural networks (NNs), <cite class="ltx_cite ltx_citemacro_citep">(Igel, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib24" title="">2003</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Salimans et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>)</cite> were the first to scale the approach up to deep learning networks. Comparable performance to RL methods in control and gaming environments was observed, and several unique advantages of ES highlighted. This seminal work paved the way for several follow-up studies. <cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib76" title="">2017</a>)</cite> used ES to optimize a convolutional NN with around three million parameters. They found that with a large enough population size, ES can approximate the performance of traditional stochastic gradient descent (SGD). <cite class="ltx_cite ltx_citemacro_citet">Lehman et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib30" title="">2018</a>)</cite> further optimized an NN comprising nearly 167,000 parameters with both ES and a traditional finite-difference (FD) gradient estimator. Because ES optimizes the average reward for the entire population, whereas FD optimizes the reward for a single solution, it obtained models that were more robust to parameter perturbations. <cite class="ltx_cite ltx_citemacro_citet">Lorenc &amp; Neruda (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib34" title="">2025</a>)</cite> applied ES to optimize decision transformers in RL environments, and observed promising results for model sizes up to around 2.5 million parameters. In a related study, another traditional EA, namely genetic algorithm (GA) with mutations only, was extended to a high-dimensional space <cite class="ltx_cite ltx_citemacro_citep">(Such et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib62" title="">2017</a>)</cite>. Encouraging results were observed in different types of models with up to around four million parameters <cite class="ltx_cite ltx_citemacro_citep">(Such et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib62" title="">2017</a>; Risi &amp; Stanley, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib45" title="">2019</a>)</cite>. However, although these studies were promising, the scale of these implementations was still significantly less than the size of current LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p">Synergies between Evolutionary Algorithms (EAs) and LLMs have received increasing attention in recent years <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib70" title="">2025</a>; Wu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib74" title="">2025</a>)</cite>. Popular research directions include EAs for prompt optimization <cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib64" title="">2022b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib63" title="">a</a>; Zhao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib78" title="">2023</a>; Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib21" title="">2024</a>)</cite>, utilizing LLMs as evolutionary operators <cite class="ltx_cite ltx_citemacro_citep">(Meyerson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib36" title="">2024</a>; Lehman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib31" title="">2024</a>; Romera-Paredes et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib46" title="">2024</a>; Novikov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib38" title="">2025</a>)</cite>, and merging LLMs through evolution <cite class="ltx_cite ltx_citemacro_citep">(Du et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib12" title="">2024</a>; Akiba et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib4" title="">2025</a>)</cite>. Applying EAs to optimize billions of parameters in LLMs is generally perceived to be intractable, but a few studies have been successful at a smaller scale. For example, <cite class="ltx_cite ltx_citemacro_citet">Toledano-López et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib66" title="">2022</a>)</cite> fine-tuned the last layer (with 325 parameters) of an mT5-based transformer via CMA-ES. <cite class="ltx_cite ltx_citemacro_citet">Jin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib27" title="">2024</a>)</cite> optimized the low-rank adapter parameters (with dimensionality up to 1600) using CMA-ES and the Fireworks algorithm. <cite class="ltx_cite ltx_citemacro_citet">Sanchez&nbsp;Carmona et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib52" title="">2024</a>)</cite> applied a GA to fine-tune around 9.5 million parameters of a transformer encoder, though poorer performance than the traditional Adam optimizer was observed. <cite class="ltx_cite ltx_citemacro_citet">Huang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib23" title="">2025</a>)</cite> proposed a hybrid algorithm that performs exploration in action space instead of parameter space, and it was only used in the final epoch of supervised fine-tuning (SFT). The work in this paper significantly extends this prior research by successfully scaling ES to search in the billions of parameters of LLMs, leading to surprisingly good fine-tuning performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p">Fine-tuning using RL is a critical step during the training of many landmark LLMs <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib39" title="">2022</a>; Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib6" title="">2022</a>; Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib57" title="">2024</a>; Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib19" title="">2025a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib20" title="">b</a>)</cite>. Proximal Policy Optimization <cite class="ltx_cite ltx_citemacro_citep">(PPO; Schulman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib54" title="">2017</a>)</cite> and Group Relative Policy Optimization <cite class="ltx_cite ltx_citemacro_citep">(GRPO; Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib57" title="">2024</a>)</cite> are the two predominant methods. PPO introduces a clipped surrogate objective to limit the update scale in each step with respect to the old policy, and it usually works with a value model in an actor-critic manner. GRPO simplifies the pipeline of PPO by replacing the value model with group advantage, which is calculated based on direct evaluations of multiple responses. As discussed in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S1" title="1 Introduction ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">1</span></a>, in the context of LLM fine-tuning, these methods struggle with several fundamental limitations, including the dilemma in handling long-horizon reward <cite class="ltx_cite ltx_citemacro_citep">(Vemula et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib69" title="">2019</a>; Salimans et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib77" title="">2025</a>; Song et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib59" title="">2025</a>; Uesato et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib68" title="">2022</a>; Jia et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib25" title="">2025</a>; Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib20" title="">2025b</a>)</cite>, sensitivity to base LLMs <cite class="ltx_cite ltx_citemacro_citep">(Gandhi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib15" title="">2025</a>)</cite>, tendency to hack reward <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib16" title="">2023</a>; Denison et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib11" title="">2024</a>; Fu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib14" title="">2025</a>)</cite>, and instability across runs <cite class="ltx_cite ltx_citemacro_citep">(Choshen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib8" title="">2020</a>; Zhong et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib79" title="">2025</a>)</cite>. ES inherently avoids these limitations, leading to better fine-tuning performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p">Existing RL fine-tuning methods are overwhelmingly based on action-space exploration.
Parameter space exploration has received much less attention, though some such studies do exist <cite class="ltx_cite ltx_citemacro_citep">(Rückstieß et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib48" title="">2008</a>; Sehnke et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib56" title="">2010</a>; Rückstieß et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib49" title="">2010</a>; Plappert et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib41" title="">2018</a>)</cite>. Although promising performance was observed in problems with sparse rewards, the scale of the tested models was far smaller than that of LLMs. <cite class="ltx_cite ltx_citemacro_citet">Vemula et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib69" title="">2019</a>)</cite> performed a theoretical analysis of different exploration strategies, and found that the complexity of the parameter space exploration increased quadratically with the number of parameters, whereas the complexity of action space exploration depended on action dimensionality quadratically and horizon length of the reward quartically. Based on the classical SPSA optimization method <cite class="ltx_cite ltx_citemacro_citep">(Spall, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib60" title="">1992</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Malladi et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib35" title="">2023</a>)</cite> proposed a zeroth-order optimizer MeZO that directly worked in parameter space for fine-tuning LLMs. MeZO significantly reduced memory requirements, but its fine-tuning performance was no better than other baselines. In contrast, the ES implementation in this paper performs exploration in multi-billion-parameter search spaces, and outperforms all baselines.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p">This section introduces the basic algorithmic structure of ES, followed by a detailed description of its implementation for LLM fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Basic ES Algorithm</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">The ES implementation in this paper is an algorithmically simplified variant of natural evolution strategies (NES) <cite class="ltx_cite ltx_citemacro_citep">(Wierstra et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib71" title="">2008</a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib72" title="">2014</a>)</cite>. The overall design is similar to OpenAI ES <cite class="ltx_cite ltx_citemacro_citep">(Salimans et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>)</cite>, which simplified NES with fixed covariance for perturbation noise.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Basic ES Algorithm</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span>Pretrained LLM with initial parameters <math alttext="\bm{\theta}_{0}" class="ltx_Math" display="inline" id="alg1.l1.m1" intent=":literal"><semantics><msub><mi>𝜽</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\bm{\theta}_{0}</annotation></semantics></math>, reward function <math alttext="R(\cdot)" class="ltx_Math" display="inline" id="alg1.l1.m2" intent=":literal"><semantics><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(\cdot)</annotation></semantics></math>, total iterations <math alttext="T" class="ltx_Math" display="inline" id="alg1.l1.m3" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, population size <math alttext="N" class="ltx_Math" display="inline" id="alg1.l1.m4" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>, noise scale <math alttext="\sigma" class="ltx_Math" display="inline" id="alg1.l1.m5" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>, learning rate <math alttext="\alpha" class="ltx_Math" display="inline" id="alg1.l1.m6" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span><span class="ltx_text ltx_font_bold">for</span> <math alttext="t=1" class="ltx_Math" display="inline" id="alg1.l2.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t=1</annotation></semantics></math> to <math alttext="T" class="ltx_Math" display="inline" id="alg1.l2.m2" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l2.m3" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> outer ES iterations
</span>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span>  <span class="ltx_text ltx_font_bold">for</span> <math alttext="n=1" class="ltx_Math" display="inline" id="alg1.l3.m1" intent=":literal"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n=1</annotation></semantics></math> to <math alttext="N" class="ltx_Math" display="inline" id="alg1.l3.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span>   Sample noise <math alttext="\bm{\varepsilon}_{n}\sim\mathcal{N}(0,\bm{I})" class="ltx_Math" display="inline" id="alg1.l4.m1" intent=":literal"><semantics><mrow><msub><mi>𝜺</mi><mi>n</mi></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\varepsilon}_{n}\sim\mathcal{N}(0,\bm{I})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>   Compute reward for perturbed parameters <math alttext="R_{n}=R(\bm{\theta}_{t-1}+\sigma\cdot\bm{\varepsilon}_{n})" class="ltx_Math" display="inline" id="alg1.l5.m1" intent=":literal"><semantics><mrow><msub><mi>R</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝜽</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mi>σ</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>𝜺</mi><mi>n</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">R_{n}=R(\bm{\theta}_{t-1}+\sigma\cdot\bm{\varepsilon}_{n})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>  <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>  Normalize <math alttext="R_{n}" class="ltx_Math" display="inline" id="alg1.l7.m1" intent=":literal"><semantics><msub><mi>R</mi><mi>n</mi></msub><annotation encoding="application/x-tex">R_{n}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>  Update model parameters as <math alttext="\bm{\theta}_{t}\leftarrow\bm{\theta}_{t-1}+\alpha\cdot\frac{1}{N}\sum_{n=1}^{N}R_{n}\bm{\varepsilon}_{n}" class="ltx_Math" display="inline" id="alg1.l8.m1" intent=":literal"><semantics><mrow><msub><mi>𝜽</mi><mi>t</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>𝜽</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>R</mi><mi>n</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝜺</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\theta}_{t}\leftarrow\bm{\theta}_{t-1}+\alpha\cdot\frac{1}{N}\sum_{n=1}^{N}R_{n}\bm{\varepsilon}_{n}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p">Given a pretrained LLM with initial parameters <math alttext="\bm{\theta}_{0}" class="ltx_Math" display="inline" id="S3.SS1.p2.m1" intent=":literal"><semantics><msub><mi>𝜽</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\bm{\theta}_{0}</annotation></semantics></math> and a target reward function <math alttext="R(\cdot)" class="ltx_Math" display="inline" id="S3.SS1.p2.m2" intent=":literal"><semantics><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(\cdot)</annotation></semantics></math>, the task is to fine-tune the parameters so that the reward function is optimized (Algorithm&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#alg1" title="Algorithm 1 ‣ 3.1 Basic ES Algorithm ‣ 3 Method ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">1</span></a>). In each iteration, <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p2.m3" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> perturbed models are sampled by adding random Gaussian noise <math alttext="\bm{\varepsilon}_{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.m4" intent=":literal"><semantics><msub><mi>𝜺</mi><mi>n</mi></msub><annotation encoding="application/x-tex">\bm{\varepsilon}_{n}</annotation></semantics></math> to their parameters. The noise is i.i.d. in each dimension of the parameter space, and it is scaled by the hyperparameter <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.SS1.p2.m5" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>. The perturbed models are evaluated to obtain their reward scores <math alttext="R_{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.m6" intent=":literal"><semantics><msub><mi>R</mi><mi>n</mi></msub><annotation encoding="application/x-tex">R_{n}</annotation></semantics></math>. The final update of the model parameters aggregates the sampled perturbations by weighting them using their normalized reward scores. The standard update equation <math alttext="\bm{\theta}_{t}\leftarrow\bm{\theta}_{t-1}+\alpha\cdot\frac{1}{\sigma}\frac{1}{N}\sum_{n=1}^{N}R_{n}\bm{\varepsilon}_{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.m7" intent=":literal"><semantics><mrow><msub><mi>𝜽</mi><mi>t</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>𝜽</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mn>1</mn><mi>σ</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>R</mi><mi>n</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝜺</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\theta}_{t}\leftarrow\bm{\theta}_{t-1}+\alpha\cdot\frac{1}{\sigma}\frac{1}{N}\sum_{n=1}^{N}R_{n}\bm{\varepsilon}_{n}</annotation></semantics></math> is simplified to <math alttext="\bm{\theta}_{t}\leftarrow\bm{\theta}_{t-1}+\alpha\cdot\frac{1}{N}\sum_{n=1}^{N}R_{n}\bm{\varepsilon}_{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.m8" intent=":literal"><semantics><mrow><msub><mi>𝜽</mi><mi>t</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>𝜽</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>R</mi><mi>n</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝜺</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\theta}_{t}\leftarrow\bm{\theta}_{t-1}+\alpha\cdot\frac{1}{N}\sum_{n=1}^{N}R_{n}\bm{\varepsilon}_{n}</annotation></semantics></math> by digesting the term <math alttext="\frac{1}{\sigma}" class="ltx_Math" display="inline" id="S3.SS1.p2.m9" intent=":literal"><semantics><mfrac><mn>1</mn><mi>σ</mi></mfrac><annotation encoding="application/x-tex">\frac{1}{\sigma}</annotation></semantics></math> into the learning rate <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS1.p2.m10" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p">To improve scalability, a number of modifications to this basic algorithm were made as detailed in the next section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Implementation Details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p">Algorithm&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#alg2" title="Algorithm 2 ‣ 3.2 Implementation Details ‣ 3 Method ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">2</span></a>, the actual implementation of ES for this paper, expands on the above algorithm in seven ways:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p">(1) <span class="ltx_text ltx_font_bold">Noise retrieval with random seeds:</span> Similar to <cite class="ltx_cite ltx_citemacro_citet">Salimans et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>); Such et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib62" title="">2017</a>)</cite>, only the random seeds are stored to reduce GPU memory usage. The perturbation noise used during sampling can be retrieved exactly by resetting the random number generator with specific random seeds. (2) <span class="ltx_text ltx_font_bold">Parallel evaluations:</span> In each iteration, the perturbed models can be evaluated fully in parallel by assigning a separate random seed to each process. (3) <span class="ltx_text ltx_font_bold">Layer-level in-place perturbation and restoration:</span> To reduce the peak GPU memory usage, the model parameters are perturbed in-place layer by layer, with corresponding random seeds archived. After evaluation of the perturbed model, the model parameters are restored by subtracting the same noise perturbations using the archived random seeds. For each evaluation process, apart from the model parameters, the only additional memory needed is to store a tensor the size of a layer temporarily. (4) <span class="ltx_text ltx_font_bold">Reward normalization:</span> The rewards of the perturbed models are normalized using <math alttext="z" class="ltx_Math" display="inline" id="S3.SS2.p2.m1" intent=":literal"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>-score within each iteration, so that the normalized rewards for each iteration have a mean of 0 and standard deviation of 1. This normalization makes the reward scale consistent across iterations and tasks. (5) <span class="ltx_text ltx_font_bold">Greedy decoding:</span> The perturbed models use greedy decoding to generate the responses for reward evaluations. As a result, the perturbed models are evaluated deterministically, so that all performance differences come from the exploration in parameter space instead of action space. (6) <span class="ltx_text ltx_font_bold">Decomposition of the parameter update:</span> At the end of each iteration, the aggregated update of model parameters is performed in-place in a decomposed manner, gradually adding up layer by layer and seed by seed, significantly reducing the peak GPU memory needed. (7) <span class="ltx_text ltx_font_bold">Learning rate digestion:</span> The standard update equation <math alttext="\bm{\theta}_{t}\leftarrow\bm{\theta}_{t-1}+\alpha\cdot\frac{1}{\sigma}\frac{1}{N}\sum_{n=1}^{N}R_{n}\bm{\varepsilon}_{n}" class="ltx_Math" display="inline" id="S3.SS2.p2.m2" intent=":literal"><semantics><mrow><msub><mi>𝜽</mi><mi>t</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>𝜽</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mn>1</mn><mi>σ</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>R</mi><mi>n</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝜺</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\theta}_{t}\leftarrow\bm{\theta}_{t-1}+\alpha\cdot\frac{1}{\sigma}\frac{1}{N}\sum_{n=1}^{N}R_{n}\bm{\varepsilon}_{n}</annotation></semantics></math> is simplified to <math alttext="\bm{\theta}_{t}\leftarrow\bm{\theta}_{t-1}+\alpha\cdot\frac{1}{N}\sum_{n=1}^{N}R_{n}\bm{\varepsilon}_{n}" class="ltx_Math" display="inline" id="S3.SS2.p2.m3" intent=":literal"><semantics><mrow><msub><mi>𝜽</mi><mi>t</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>𝜽</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>R</mi><mi>n</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝜺</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\theta}_{t}\leftarrow\bm{\theta}_{t-1}+\alpha\cdot\frac{1}{N}\sum_{n=1}^{N}R_{n}\bm{\varepsilon}_{n}</annotation></semantics></math> by digesting the term <math alttext="\frac{1}{\sigma}" class="ltx_Math" display="inline" id="S3.SS2.p2.m4" intent=":literal"><semantics><mfrac><mn>1</mn><mi>σ</mi></mfrac><annotation encoding="application/x-tex">\frac{1}{\sigma}</annotation></semantics></math> into the learning rate <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS2.p2.m5" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>, simplifying the computation and parametric setup.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p">In order to keep the algorithm simple, common enhancements in OpenAI ES <cite class="ltx_cite ltx_citemacro_citet">Salimans et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>)</cite> such as rank transformation of rewards <cite class="ltx_cite ltx_citemacro_citep">(Wierstra et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib72" title="">2014</a>)</cite>, mirrored sampling <cite class="ltx_cite ltx_citemacro_citep">(Sehnke et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib56" title="">2010</a>)</cite>, weight decay, and virtual batch normalization <cite class="ltx_cite ltx_citemacro_citep">(Salimans et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib50" title="">2016</a>)</cite> are not used, and neither are more advanced optimizers like Adam <cite class="ltx_cite ltx_citemacro_citep">(Kingma &amp; Ba, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib28" title="">2015</a>)</cite>. They can be included in to improve results in future work.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 2</span> </span> ES Implementation for LLM Fine-Tuning</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg2.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span>Pretrained LLM with initial parameters <math alttext="\bm{\theta}_{0}" class="ltx_Math" display="inline" id="alg2.l1.m1" intent=":literal"><semantics><msub><mi>𝜽</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\bm{\theta}_{0}</annotation></semantics></math>, reward function <math alttext="R(\cdot)" class="ltx_Math" display="inline" id="alg2.l1.m2" intent=":literal"><semantics><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(\cdot)</annotation></semantics></math>, total iterations <math alttext="T" class="ltx_Math" display="inline" id="alg2.l1.m3" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, population size <math alttext="N" class="ltx_Math" display="inline" id="alg2.l1.m4" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>, noise scale <math alttext="\sigma" class="ltx_Math" display="inline" id="alg2.l1.m5" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>, learning rate <math alttext="\alpha" class="ltx_Math" display="inline" id="alg2.l1.m6" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>, number of parallel process <math alttext="P" class="ltx_Math" display="inline" id="alg2.l1.m7" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg2.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span>Create <math alttext="P" class="ltx_Math" display="inline" id="alg2.l2.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> processes, each instantiates a model with the same initial parameters <math alttext="\bm{\theta}_{0}" class="ltx_Math" display="inline" id="alg2.l2.m2" intent=":literal"><semantics><msub><mi>𝜽</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\bm{\theta}_{0}</annotation></semantics></math>, with one process as the main process

</div>
<div class="ltx_listingline" id="alg2.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span><span class="ltx_text ltx_font_bold">for</span> <math alttext="t=1" class="ltx_Math" display="inline" id="alg2.l3.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t=1</annotation></semantics></math> to <math alttext="T" class="ltx_Math" display="inline" id="alg2.l3.m2" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l3.m3" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> ES iterations
</span>
</div>
<div class="ltx_listingline" id="alg2.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span>  Sample N random seeds <math alttext="s_{1},s_{2},\ldots,s_{N}" class="ltx_Math" display="inline" id="alg2.l4.m1" intent=":literal"><semantics><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>s</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">s_{1},s_{2},\ldots,s_{N}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>  Assign random seeds to <math alttext="P" class="ltx_Math" display="inline" id="alg2.l5.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> processes

</div>
<div class="ltx_listingline" id="alg2.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>  <span class="ltx_text ltx_font_bold">for</span> <math alttext="n=1" class="ltx_Math" display="inline" id="alg2.l6.m1" intent=":literal"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n=1</annotation></semantics></math> to <math alttext="N" class="ltx_Math" display="inline" id="alg2.l6.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg2.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>   For the process handling <math alttext="s_{n}" class="ltx_Math" display="inline" id="alg2.l7.m1" intent=":literal"><semantics><msub><mi>s</mi><mi>n</mi></msub><annotation encoding="application/x-tex">s_{n}</annotation></semantics></math>, reset its random number generator using random seed <math alttext="s_{n}" class="ltx_Math" display="inline" id="alg2.l7.m2" intent=":literal"><semantics><msub><mi>s</mi><mi>n</mi></msub><annotation encoding="application/x-tex">s_{n}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>   <span class="ltx_text ltx_font_bold">for</span> <span class="ltx_text ltx_font_bold">each LLM layer</span> <span class="ltx_text ltx_font_bold">do</span> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l8.m1" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> perturbation within current process
</span>
</div>
<div class="ltx_listingline" id="alg2.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span>     Sample noise <math alttext="\bm{\varepsilon}_{n,l}\sim\mathcal{N}(0,\bm{I})" class="ltx_Math" display="inline" id="alg2.l9.m1" intent=":literal"><semantics><mrow><msub><mi>𝜺</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\varepsilon}_{n,l}\sim\mathcal{N}(0,\bm{I})</annotation></semantics></math>, which has the same shape as the <math alttext="l" class="ltx_Math" display="inline" id="alg2.l9.m2" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>th layer’s parameters

</div>
<div class="ltx_listingline" id="alg2.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span>     Perturb the <math alttext="l" class="ltx_Math" display="inline" id="alg2.l10.m1" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>th layer’s parameters in-place: <math alttext="\bm{\theta}_{t-1,l}\leftarrow\bm{\theta}_{t-1,l}+\sigma\cdot\bm{\varepsilon}_{n,l}" class="ltx_Math" display="inline" id="alg2.l10.m2" intent=":literal"><semantics><mrow><msub><mi>𝜽</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo stretchy="false">←</mo><mrow><msub><mi>𝜽</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo>+</mo><mrow><mi>σ</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>𝜺</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\theta}_{t-1,l}\leftarrow\bm{\theta}_{t-1,l}+\sigma\cdot\bm{\varepsilon}_{n,l}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span>   <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg2.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span>   Compute reward for perturbed parameters <math alttext="R_{n}=R(\bm{\theta}_{t-1})" class="ltx_Math" display="inline" id="alg2.l12.m1" intent=":literal"><semantics><mrow><msub><mi>R</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝜽</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">R_{n}=R(\bm{\theta}_{t-1})</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l12.m2" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> within current process
</span>
</div>
<div class="ltx_listingline" id="alg2.l13">
<span class="ltx_tag ltx_tag_listingline">13:</span>   For the process handling <math alttext="s_{n}" class="ltx_Math" display="inline" id="alg2.l13.m1" intent=":literal"><semantics><msub><mi>s</mi><mi>n</mi></msub><annotation encoding="application/x-tex">s_{n}</annotation></semantics></math>, reset its random number generator using random seed <math alttext="s_{n}" class="ltx_Math" display="inline" id="alg2.l13.m2" intent=":literal"><semantics><msub><mi>s</mi><mi>n</mi></msub><annotation encoding="application/x-tex">s_{n}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l14">
<span class="ltx_tag ltx_tag_listingline">14:</span>   <span class="ltx_text ltx_font_bold">for</span> <span class="ltx_text ltx_font_bold">each LLM layer</span> <span class="ltx_text ltx_font_bold">do</span> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l14.m1" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> restoration within current process
</span>
</div>
<div class="ltx_listingline" id="alg2.l15">
<span class="ltx_tag ltx_tag_listingline">15:</span>     Sample noise <math alttext="\bm{\varepsilon}_{n,l}\sim\mathcal{N}(0,\bm{I})" class="ltx_Math" display="inline" id="alg2.l15.m1" intent=":literal"><semantics><mrow><msub><mi>𝜺</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\varepsilon}_{n,l}\sim\mathcal{N}(0,\bm{I})</annotation></semantics></math>, which has the same shape as the <math alttext="l" class="ltx_Math" display="inline" id="alg2.l15.m2" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>th layer’s parameters

</div>
<div class="ltx_listingline" id="alg2.l16">
<span class="ltx_tag ltx_tag_listingline">16:</span>     Restore the <math alttext="l" class="ltx_Math" display="inline" id="alg2.l16.m1" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>th layer’s parameters in-place: <math alttext="\bm{\theta}_{t-1,l}\leftarrow\bm{\theta}_{t-1,l}-\sigma\cdot\bm{\varepsilon}_{n,l}" class="ltx_Math" display="inline" id="alg2.l16.m2" intent=":literal"><semantics><mrow><msub><mi>𝜽</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo stretchy="false">←</mo><mrow><msub><mi>𝜽</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo>−</mo><mrow><mi>σ</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>𝜺</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\theta}_{t-1,l}\leftarrow\bm{\theta}_{t-1,l}-\sigma\cdot\bm{\varepsilon}_{n,l}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l17">
<span class="ltx_tag ltx_tag_listingline">17:</span>   <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg2.l18">
<span class="ltx_tag ltx_tag_listingline">18:</span>  <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg2.l19">
<span class="ltx_tag ltx_tag_listingline">19:</span>  Normalize the reward scores by calculating the <math alttext="z" class="ltx_Math" display="inline" id="alg2.l19.m1" intent=":literal"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>-score for each <math alttext="R_{n}" class="ltx_Math" display="inline" id="alg2.l19.m2" intent=":literal"><semantics><msub><mi>R</mi><mi>n</mi></msub><annotation encoding="application/x-tex">R_{n}</annotation></semantics></math>: <math alttext="Z_{n}=\frac{R_{n}-R_{\mathrm{mean}}}{R_{\mathrm{std}}}" class="ltx_Math" display="inline" id="alg2.l19.m3" intent=":literal"><semantics><mrow><msub><mi>Z</mi><mi>n</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>R</mi><mi>n</mi></msub><mo>−</mo><msub><mi>R</mi><mi>mean</mi></msub></mrow><msub><mi>R</mi><mi>std</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">Z_{n}=\frac{R_{n}-R_{\mathrm{mean}}}{R_{\mathrm{std}}}</annotation></semantics></math>,

</div>
<div class="ltx_listingline" id="alg2.l20">
<span class="ltx_tag ltx_tag_listingline">20:</span>  where <math alttext="R_{\mathrm{mean}}" class="ltx_Math" display="inline" id="alg2.l20.m1" intent=":literal"><semantics><msub><mi>R</mi><mi>mean</mi></msub><annotation encoding="application/x-tex">R_{\mathrm{mean}}</annotation></semantics></math> and <math alttext="R_{\mathrm{std}}" class="ltx_Math" display="inline" id="alg2.l20.m2" intent=":literal"><semantics><msub><mi>R</mi><mi>std</mi></msub><annotation encoding="application/x-tex">R_{\mathrm{std}}</annotation></semantics></math> are the mean and standard deviation of <math alttext="R_{1},R_{2},\ldots,R_{N}" class="ltx_Math" display="inline" id="alg2.l20.m3" intent=":literal"><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub><mo>,</mo><msub><mi>R</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>R</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">R_{1},R_{2},\ldots,R_{N}</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg2.l21">
<span class="ltx_tag ltx_tag_listingline">21:</span>  <span class="ltx_text ltx_font_bold">for</span> <math alttext="n=1" class="ltx_Math" display="inline" id="alg2.l21.m1" intent=":literal"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n=1</annotation></semantics></math> to <math alttext="N" class="ltx_Math" display="inline" id="alg2.l21.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l21.m3" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> in main process only
</span>
</div>
<div class="ltx_listingline" id="alg2.l22">
<span class="ltx_tag ltx_tag_listingline">22:</span>   Reset current random number generator using random seed <math alttext="s_{n}" class="ltx_Math" display="inline" id="alg2.l22.m1" intent=":literal"><semantics><msub><mi>s</mi><mi>n</mi></msub><annotation encoding="application/x-tex">s_{n}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l23">
<span class="ltx_tag ltx_tag_listingline">23:</span>   <span class="ltx_text ltx_font_bold">for</span> <span class="ltx_text ltx_font_bold">each LLM layer</span> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg2.l24">
<span class="ltx_tag ltx_tag_listingline">24:</span>     Sample noise <math alttext="\bm{\varepsilon}_{n,l}\sim\mathcal{N}(0,\bm{I})" class="ltx_Math" display="inline" id="alg2.l24.m1" intent=":literal"><semantics><mrow><msub><mi>𝜺</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\varepsilon}_{n,l}\sim\mathcal{N}(0,\bm{I})</annotation></semantics></math>, which has the same shape as the <math alttext="l" class="ltx_Math" display="inline" id="alg2.l24.m2" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>th layer’s parameters

</div>
<div class="ltx_listingline" id="alg2.l25">
<span class="ltx_tag ltx_tag_listingline">25:</span>     Update <math alttext="l" class="ltx_Math" display="inline" id="alg2.l25.m1" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>th layer’s parameters in-place as <math alttext="\bm{\theta}_{t,l}\leftarrow\bm{\theta}_{t-1,l}+\alpha\cdot\frac{1}{N}Z_{n}\bm{\varepsilon}_{n,l}" class="ltx_Math" display="inline" id="alg2.l25.m2" intent=":literal"><semantics><mrow><msub><mi>𝜽</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo stretchy="false">←</mo><mrow><msub><mi>𝜽</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo>+</mo><mrow><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>Z</mi><mi>n</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝜺</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\theta}_{t,l}\leftarrow\bm{\theta}_{t-1,l}+\alpha\cdot\frac{1}{N}Z_{n}\bm{\varepsilon}_{n,l}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l26">
<span class="ltx_tag ltx_tag_listingline">26:</span>   <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg2.l27">
<span class="ltx_tag ltx_tag_listingline">27:</span>  <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg2.l28">
<span class="ltx_tag ltx_tag_listingline">28:</span>  Update the model parameters of all processes to <math alttext="\bm{\theta}_{t}" class="ltx_Math" display="inline" id="alg2.l28.m1" intent=":literal"><semantics><msub><mi>𝜽</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{\theta}_{t}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l29">
<span class="ltx_tag ltx_tag_listingline">29:</span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Empirical Studies</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p">This section first compares the fine-tuning performance of ES, PPO and GRPO on a standard reasoning benchmark. After that, behavioral differences between ES and RL are investigated in fine-tuning for conciseness in the next section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Performance in the Countdown Task</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p">Fine-tuning performance was measured in the Countdown task&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib40" title="">2025</a>; Goodfellow et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib17" title="">2016</a>)</cite>, a symbolic reasoning benchmark, showing that ES is accurate and sample efficent across different kinds and sizes of LLMs, even when the RL approaches is not.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Countdown task.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p">The Countdown task&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib40" title="">2025</a>; Goodfellow et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib17" title="">2016</a>)</cite> requires constructing an arithmetic expression from a given set of numbers using basic operations <math alttext="(+,-,\times,\div)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">+</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">−</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">×</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">÷</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(+,-,\times,\div)</annotation></semantics></math> to match a target value. For instance, the target <math alttext="950" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m2" intent=":literal"><semantics><mn>950</mn><annotation encoding="application/x-tex">950</annotation></semantics></math> can be obtained from <math alttext="\{100,50,6,3\}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m3" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mn>100</mn><mo>,</mo><mn>50</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>3</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{100,50,6,3\}</annotation></semantics></math> with <math alttext="100\times(6+3)+50=950" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m4" intent=":literal"><semantics><mrow><mrow><mrow><mn>100</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mrow><mo stretchy="false">(</mo><mrow><mn>6</mn><mo>+</mo><mn>3</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mn>50</mn></mrow><mo>=</mo><mn>950</mn></mrow><annotation encoding="application/x-tex">100\times(6+3)+50=950</annotation></semantics></math>). This constitutes a compact test of constrained symbolic reasoning, i.e. an important use case for fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Base Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Original</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">RL</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">ES (ours)</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">PPO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold">GRPO (8)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold">GRPO (30)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" style="--ltx-bg-color:#F7FFF7;">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">Qwen-2.5-0.5B-Instruct</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">0.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">0.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">0.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">0.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#F7FFF7;">14.4</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7FFF7;">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">Qwen-2.5-1.5B-Instruct</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">0.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">14.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">13.9</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">14.8</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#F7FFF7;">37.3</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7FFF7;">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">Qwen-2.5-3B-Instruct</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">10.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">20.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">30.9</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">32.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#F7FFF7;">60.5</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7FFF7;">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">Qwen-2.5-7B-Instruct</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">31.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">55.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">54.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">52.8</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#F7FFF7;">66.8</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7F7FF;">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">LLaMA-3.2-1B-Instruct</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">0.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">11.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">14.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">13.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#F7F7FF;">16.8</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7F7FF;">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">LLaMA-3.2-3B-Instruct</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">3.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">35.3</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">39.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">38.8</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#F7F7FF;">51.6</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7F7FF;">
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">LLaMA-3.1-8B-Instruct</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">8.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">42.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">49.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">51.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#F7F7FF;">61.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy (%) on the Countdown task across model families, sizes, and fine-tuning algorithms. Different model families are shaded for clarity; <span class="ltx_text ltx_font_italic">Original</span> refers to directly evaluating the base model without any fine-tuning, and GRPO (8) and GRPO (30) indicate group sizes of 8 and 30. The same hyperparameters were used for all ES runs; a separate grid search for the best hyperparameters was run for each RL experiment.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Experimental Setup.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p">A single fixed set of hyperparameters was used for all ES experiments. For RL, a separate hyperparameter sweep was done for each experiment. RL methods turned out sensitive to hyperparameters, in particular the KL-divergence penalty coefficient <math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m1" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> and learning rate <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m2" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>, and did not make much progress if they were not set precisely. To mitigate this issue, for each model, a small grid of <math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m3" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> and <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m4" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> values were tested and the best-performing configuration selected (further details are provided in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.T3" title="Table 3 ‣ Experimental setup for the Countdown experiments. ‣ A.1 Experimental Setup ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">3</span></a> in the Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS1" title="A.1 Experimental Setup ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">A.1</span></a>). This approach makes the comparison conservative with respect to ES, but it also highlights its robustness.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">ES improves upon PPO and GRPO across all tested models.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p">Previously, <cite class="ltx_cite ltx_citemacro_citet">Gandhi et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib15" title="">2025</a>)</cite> found that RL does not generalize well across models on the Countdown task. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.T1" title="Table 1 ‣ Countdown task. ‣ 4.1 Performance in the Countdown Task ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">1</span></a> confirms this result, and also demonstrates that ES does not have this problem. With each model in the Qwen2.5 family (0.5B–7B) and the LLaMA3 family (1B–8B), ES substantially improved over both PPO and GRPO, often by a large margin. Averaged across all models, ES improves over the base model by 36.4%, compared to 17.9% for PPO and 21.3% for GRPO with group size <math alttext="N=8" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m1" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">N=8</annotation></semantics></math> and 21.4% for group size <math alttext="N=30" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m2" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">N=30</annotation></semantics></math> (see Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F5" title="Figure 5 ‣ GRPO with different learning rates. ‣ A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">5</span></a> in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS4" title="A.4 Training Curves and Accuracy Improvement of ES and RL on the Countdown Task ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">A.4</span></a> for a model-wise visual comparison). These results demonstrate that ES scales effectively across different model types and sizes, and does so significantly better than RL.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">ES is more sample efficient than RL.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px4.p1">
<p class="ltx_p">Surprisingly, even when searching in a space with billions of parameters, ES is more sample efficient than the RL methods. The experiments reported in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.T1" title="Table 1 ‣ Countdown task. ‣ 4.1 Performance in the Countdown Task ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">1</span></a> were all run with the same total number of training sample evaluations (each training sample is one Countdown question). Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F6" title="Figure 6 ‣ A.4 Training Curves and Accuracy Improvement of ES and RL on the Countdown Task ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">6</span></a> in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS4" title="A.4 Training Curves and Accuracy Improvement of ES and RL on the Countdown Task ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">A.4</span></a> further shows the learning curves; to reach the same level of fine-tuning performance as RL, ES needs less than <math alttext="20\%" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.m1" intent=":literal"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">20\%</annotation></semantics></math> of the training sample evaluations in most cases. This observation is different from the that of <cite class="ltx_cite ltx_citemacro_citet">Salimans et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>)</cite>, who found worse sample efficiency compared to RL. One critical factor is the population size <math alttext="N" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>; the experiments in this paper had <math alttext="N=30" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.m3" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">N=30</annotation></semantics></math>, whereas <cite class="ltx_cite ltx_citemacro_citet">Salimans et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>)</cite> used <math alttext="N=10,000" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.m4" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N=10,000</annotation></semantics></math>, which may explain the difference.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">ES is effective on smaller models.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px5.p1">
<p class="ltx_p">Prior work on DeepSeek-R1&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib20" title="">2025b</a>)</cite> and Tiny-Zero&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib40" title="">2025</a>)</cite> pointed out a key limitation of PPO and GRPO: they require sufficient large base models to improve. For instance, <cite class="ltx_cite ltx_citemacro_citet">Pan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib40" title="">2025</a>)</cite> note that “for Qwen2.5-0.5B base, we know it fails to learn reasoning.” Surprisingly, ES overcomes this limitation. As shown in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.T1" title="Table 1 ‣ Countdown task. ‣ 4.1 Performance in the Countdown Task ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">1</span></a>, while PPO and GRPO indeed obtain only 0.3% accuracy on that model, ES boosts accuracy to 14.4%, thus eliciting reasoning even from the smallest-scale base model. This performance difference demonstrates the benefit of parameter-space exploration in ES: while RL cannot find better actions from the limited initial model to bootstrap learning, ES modifies the model directly by adding perturbations in parameter space, possibly creating better models to facilitate further exploration. These results highlight a distinct advantage of ES: it is able to improve behavior even with smaller, weaker base models, thus expanding the scope of fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Behavioral Differences between ES and RL in fine-tuning for conciseness</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p">In order to characterize the different approaches that ES and RL take, they were used to fine-tune Qwen-2.5-7B Instruct, towards more concise responses in question-answering. That is, fine-tuning was rewarded based on how concise the answers were, but not directly rewarded for its question-answering performance. In this setup, it was possible to analyze not only whether fine-tuning was effective, but also how it was achieved, including what its side effects were.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Conciseness task.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p">For conciseness fine-tuning, a dataset of prompts <math alttext="\mathcal{D}=\{x_{1},..,x_{K}\}" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo lspace="0em" rspace="0.0835em">.</mo><mo lspace="0.0835em" rspace="0.167em">.</mo><mo>,</mo><msub><mi>x</mi><mi>K</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}=\{x_{1},..,x_{K}\}</annotation></semantics></math>, with a set of verifiable solutions <math alttext="\{s_{1},...,s_{K}\}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m2" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>s</mi><mi>K</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{s_{1},...,s_{K}\}</annotation></semantics></math>, i.e. shortest possible correct answers, was used. For example, for the prompt “Name one primary color”, possible shortest verifiable solution used is “Red”. Following this approach, for each prompt <math alttext="x\in\mathcal{D}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m3" intent=":literal"><semantics><mrow><mi>x</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi></mrow><annotation encoding="application/x-tex">x\in\mathcal{D}</annotation></semantics></math>, the model was encouraged to generate a concise response <math alttext="y" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m4" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>. To fine-tune the model to generate concise responses, a reward computed using the absolute length difference between the generated response <math alttext="y" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m5" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> and the corresponding verified solution <math alttext="s_{k}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m6" intent=":literal"><semantics><msub><mi>s</mi><mi>k</mi></msub><annotation encoding="application/x-tex">s_{k}</annotation></semantics></math> was given to the model for each prompt <math alttext="x_{k}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m7" intent=":literal"><semantics><msub><mi>x</mi><mi>k</mi></msub><annotation encoding="application/x-tex">x_{k}</annotation></semantics></math>. The reward function <math alttext="R" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m8" intent=":literal"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> for conciseness was defined as <math alttext="R=-\lvert\mathrm{len}(y)-\mathrm{len}(s_{k})\rvert" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m9" intent=":literal"><semantics><mrow><mi>R</mi><mo>=</mo><mrow><mo>−</mo><mrow><mo stretchy="false">|</mo><mrow><mrow><mi>len</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>len</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">|</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">R=-\lvert\mathrm{len}(y)-\mathrm{len}(s_{k})\rvert</annotation></semantics></math>, where <math alttext="\mathrm{len}(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m10" intent=":literal"><semantics><mrow><mi>len</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{len}(\cdot)</annotation></semantics></math> denotes the string length.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.F1.fig1" style="width:212.5pt;">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:212.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="498" id="S4.F1.g1" src="./es_files/x1.png" width="830">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Mean conciseness reward and mean KL divergence from the base model for each fine-tuning checkpoint across different learning parameters. The Pareto front of ES (blue line) is higher and to the left of the GRPO Pareto front (black line) models, indicating that it found better tradeoffs. ES discovers these solutions without any KL divergence penalty, suggesting that it represents a distinctly different fine-tuning mechanism from the GRPO.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Behavior metrics.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p">Behavior of the fine-tuned models was measured in two ways: the mean conciseness reward and the mean KL divergence from the base model <cite class="ltx_cite ltx_citemacro_citep">(after  Rafailov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib43" title="">2023</a>)</cite>. KL divergence is useful as a proxy for the preservation of the base model’s behavior. It correlates strongly with the question-answering performance of the model, but also conveys more information, i.e. the extent of the fine-tuning changes. A low KL divergence thus suggests that the fine-tuned model has not forgotten capabilities learned during pre-training. Further, as KL divergence increases, these capabilities are likely to break. Therefore, fine-tuning behavior can be characterized using the tradeoffs between reward and KL divergence. To compute the metrics, each fine-tuned model was evaluated on a set of held-out test prompts, with <math alttext="20" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.m1" intent=":literal"><semantics><mn>20</mn><annotation encoding="application/x-tex">20</annotation></semantics></math> responses sampled per prompt. The reward was computed using the model-generated response and the verifiable solution provided in the test dataset. The KL divergence between a fine-tuned model <math alttext="\theta_{\mathrm{FT}}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.m2" intent=":literal"><semantics><msub><mi>θ</mi><mi>FT</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{FT}}</annotation></semantics></math> and a base model <math alttext="\theta_{\mathrm{BASE}}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.m3" intent=":literal"><semantics><msub><mi>θ</mi><mi>BASE</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{BASE}}</annotation></semantics></math> for a given prompt <math alttext="x" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.m4" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and corresponding response <math alttext="y" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.m5" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> was approximated following <cite class="ltx_cite ltx_citemacro_citet">Schulman (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib53" title="">2020</a>)</cite> as</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{KL}\!\left[\theta_{\mathrm{FT}}\,\|\,\theta_{\mathrm{BASE}}\right]=\frac{\theta_{\mathrm{BASE}}(y_{i,t}\mid x,y_{i,&lt;t})}{\theta_{\mathrm{FT}}(y_{i,t}\mid x,y_{i,&lt;t})}-\log\frac{\theta_{\mathrm{BASE}}(y_{i,t}\mid x,y_{i,&lt;t})}{\theta_{\mathrm{FT}}(y_{i,t}\mid x,y_{i,&lt;t})}-1." class="ltx_Math" display="block" id="S4.E1.m1" intent=":literal"><semantics><mrow><mrow><mrow><mpadded style="width:1.431em;" width="1.431em"><mi>KL</mi></mpadded><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msub><mi>θ</mi><mi>FT</mi></msub><mo rspace="0.448em">∥</mo><msub><mi>θ</mi><mi>BASE</mi></msub></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mrow><msub><mi>θ</mi><mi>BASE</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>x</mi><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>θ</mi><mi>FT</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>x</mi><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo>−</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mrow><msub><mi>θ</mi><mi>BASE</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>x</mi><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>θ</mi><mi>FT</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>x</mi><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo>−</mo><mn>1</mn></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathrm{KL}\!\left[\theta_{\mathrm{FT}}\,\|\,\theta_{\mathrm{BASE}}\right]=\frac{\theta_{\mathrm{BASE}}(y_{i,t}\mid x,y_{i,&lt;t})}{\theta_{\mathrm{FT}}(y_{i,t}\mid x,y_{i,&lt;t})}-\log\frac{\theta_{\mathrm{BASE}}(y_{i,t}\mid x,y_{i,&lt;t})}{\theta_{\mathrm{FT}}(y_{i,t}\mid x,y_{i,&lt;t})}-1.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">ES discovers a dominant Pareto front.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p">Similarly to <cite class="ltx_cite ltx_citemacro_cite">Rafailov et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib43" title="">2023</a>)</cite>, a Pareto frontier analysis was used to compare ES and GRPO, with mean reward and mean KL divergence as the metrics (Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.F1.fig1" title="Figure 1 ‣ Conciseness task. ‣ 4.2 Behavioral Differences between ES and RL in fine-tuning for conciseness ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">1</span></a>). The experimental setup is described in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS1" title="A.1 Experimental Setup ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">A.1</span></a>.
The ES Pareto front is represented by a blue line on top and the GRPO Pareto front by the black line below. That is, ES produced better tradeoffs than GRPO, i.e. models with higher reward and lower KL divergence. The GRPO results were achieved only after augmenting the conciseness reward with a KL divergence penalty (weighted by a parameter <math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.m1" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>). Without it, fine-tuning resulted in excessive divergence and incorrect answers. Remarkably, ES achieved superior tradeoffs without any KL divergence penalty, suggesting that ES fine-tuning is based on discovering distinctly different kinds of solutions than GRPO. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS3" title="A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">A.3</span></a> presents additional experiments with varying <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.m2" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.m3" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> values, yielding similar conclusions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">ES is more robust against reward hacking.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px4.p1">
<p class="ltx_p">GRPO with <math alttext="\beta=\{0.0,0.01\}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px4.p1.m1" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>0.0</mn><mo>,</mo><mn>0.01</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\beta=\{0.0,0.01\}</annotation></semantics></math> sometimes hacked the reward, that is, produced responses that were short but contain nonsensical symbols rather than words. By increasing the KL-penalty via higher <math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px4.p1.m2" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> values, reward hacking could be prevented. The optimal <math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px4.p1.m3" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> is likely to be problem specific and to require extensive search to find. In contrast, ES does not receive any feedback about the divergence of the fine-tuned model, and only seeks to optimize conciseness. Regardless, it did not exhibit any reward hacking, despite achieving mean reward comparable to GRPO with <math alttext="\beta=\{0.0,0.01\}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px4.p1.m4" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>0.0</mn><mo>,</mo><mn>0.01</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\beta=\{0.0,0.01\}</annotation></semantics></math>. This result again suggests that ES finds a different way of optimizing the reward function.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math alttext="\beta" class="ltx_Math" display="inline" id="S4.T2.m1" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math alttext="\alpha" class="ltx_Math" display="inline" id="S4.T2.m2" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math alttext="\sigma" class="ltx_Math" display="inline" id="S4.T2.m3" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span class="ltx_text ltx_font_bold">Reward</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.m4" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span class="ltx_text ltx_font_bold">KL</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.m5" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" style="--ltx-bg-color:#F7FFF7;">
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">Qwen-2.5-7B+GRPO</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">0.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math alttext="5\times 10^{-6}" class="ltx_Math" display="inline" id="S4.T2.m6" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">5</mn><mo lspace="0.222em" mathbackground="#F7FFF7" rspace="0.222em" style="--ltx-bg-color:#F7FFF7;">×</mo><msup><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">10</mn><mrow><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">−</mo><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\times 10^{-6}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math alttext="0.867\pm 0.054^{*}" class="ltx_Math" display="inline" id="S4.T2.m7" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.867</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">±</mo><msup><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.054</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">∗</mo></msup></mrow><annotation encoding="application/x-tex">0.867\pm 0.054^{*}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math alttext="0.861\pm 0.614^{*}" class="ltx_Math" display="inline" id="S4.T2.m8" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.861</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">±</mo><msup><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.614</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">∗</mo></msup></mrow><annotation encoding="application/x-tex">0.861\pm 0.614^{*}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7FFF7;">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">Qwen-2.5-7B+GRPO</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">0.01</span></td>
<td class="ltx_td ltx_align_center"><math alttext="5\times 10^{-6}" class="ltx_Math" display="inline" id="S4.T2.m9" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">5</mn><mo lspace="0.222em" mathbackground="#F7FFF7" rspace="0.222em" style="--ltx-bg-color:#F7FFF7;">×</mo><msup><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">10</mn><mrow><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">−</mo><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\times 10^{-6}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">✗</span></td>
<td class="ltx_td ltx_align_center"><math alttext="0.871\pm 0.060^{*}" class="ltx_Math" display="inline" id="S4.T2.m10" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.871</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">±</mo><msup><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.060</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">∗</mo></msup></mrow><annotation encoding="application/x-tex">0.871\pm 0.060^{*}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="1.354\pm 0.873^{*}" class="ltx_Math" display="inline" id="S4.T2.m11" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">1.354</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">±</mo><msup><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.873</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">∗</mo></msup></mrow><annotation encoding="application/x-tex">1.354\pm 0.873^{*}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7FFF7;">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">Qwen-2.5-7B+GRPO</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">0.0167</span></td>
<td class="ltx_td ltx_align_center"><math alttext="5\times 10^{-6}" class="ltx_Math" display="inline" id="S4.T2.m12" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">5</mn><mo lspace="0.222em" mathbackground="#F7FFF7" rspace="0.222em" style="--ltx-bg-color:#F7FFF7;">×</mo><msup><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">10</mn><mrow><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">−</mo><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\times 10^{-6}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">✗</span></td>
<td class="ltx_td ltx_align_center"><math alttext="0.911\pm 0.038" class="ltx_Math" display="inline" id="S4.T2.m13" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.911</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">±</mo><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.038</mn></mrow><annotation encoding="application/x-tex">0.911\pm 0.038</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="1.591\pm 0.811" class="ltx_Math" display="inline" id="S4.T2.m14" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">1.591</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">±</mo><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.811</mn></mrow><annotation encoding="application/x-tex">1.591\pm 0.811</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7FFF7;">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">Qwen-2.5-7B+GRPO</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">0.0464</span></td>
<td class="ltx_td ltx_align_center"><math alttext="5\times 10^{-6}" class="ltx_Math" display="inline" id="S4.T2.m15" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">5</mn><mo lspace="0.222em" mathbackground="#F7FFF7" rspace="0.222em" style="--ltx-bg-color:#F7FFF7;">×</mo><msup><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">10</mn><mrow><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">−</mo><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\times 10^{-6}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#F7FFF7;">✗</span></td>
<td class="ltx_td ltx_align_center"><math alttext="0.881\pm 0.062" class="ltx_Math" display="inline" id="S4.T2.m16" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.881</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">±</mo><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">0.062</mn></mrow><annotation encoding="application/x-tex">0.881\pm 0.062</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="1.384\pm 1.187" class="ltx_Math" display="inline" id="S4.T2.m17" intent=":literal" style="--ltx-bg-color:#F7FFF7;"><semantics><mrow><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">1.384</mn><mo mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">±</mo><mn mathbackground="#F7FFF7" style="--ltx-bg-color:#F7FFF7;">1.187</mn></mrow><annotation encoding="application/x-tex">1.384\pm 1.187</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7F7FF;">
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">Qwen-2.5-7B+ES</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">✗</span></td>
<td class="ltx_td ltx_align_center"><math alttext="0.0005" class="ltx_Math" display="inline" id="S4.T2.m18" intent=":literal" style="--ltx-bg-color:#F7F7FF;"><semantics><mn mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.0005</mn><annotation encoding="application/x-tex">0.0005</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="0.001" class="ltx_Math" display="inline" id="S4.T2.m19" intent=":literal" style="--ltx-bg-color:#F7F7FF;"><semantics><mn mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.001</mn><annotation encoding="application/x-tex">0.001</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="0.889\pm\textbf{0.004}" class="ltx_Math" display="inline" id="S4.T2.m20" intent=":literal" style="--ltx-bg-color:#F7F7FF;"><semantics><mrow><mn mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.889</mn><mo mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">±</mo><mtext class="ltx_mathvariant_bold" mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.004</mtext></mrow><annotation encoding="application/x-tex">0.889\pm\textbf{0.004}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="0.274\pm\textbf{0.096}" class="ltx_Math" display="inline" id="S4.T2.m21" intent=":literal" style="--ltx-bg-color:#F7F7FF;"><semantics><mrow><mn mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.274</mn><mo mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">±</mo><mtext class="ltx_mathvariant_bold" mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.096</mtext></mrow><annotation encoding="application/x-tex">0.274\pm\textbf{0.096}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#F7F7FF;">
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">Qwen-2.5-7B+ES</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#F7F7FF;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math alttext="0.00075" class="ltx_Math" display="inline" id="S4.T2.m22" intent=":literal" style="--ltx-bg-color:#F7F7FF;"><semantics><mn mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.00075</mn><annotation encoding="application/x-tex">0.00075</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math alttext="0.0015" class="ltx_Math" display="inline" id="S4.T2.m23" intent=":literal" style="--ltx-bg-color:#F7F7FF;"><semantics><mn mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.0015</mn><annotation encoding="application/x-tex">0.0015</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math alttext="0.919\pm\textbf{0.008}" class="ltx_Math" display="inline" id="S4.T2.m24" intent=":literal" style="--ltx-bg-color:#F7F7FF;"><semantics><mrow><mn mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.919</mn><mo mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">±</mo><mtext class="ltx_mathvariant_bold" mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.008</mtext></mrow><annotation encoding="application/x-tex">0.919\pm\textbf{0.008}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math alttext="0.813\pm\textbf{0.212}" class="ltx_Math" display="inline" id="S4.T2.m25" intent=":literal" style="--ltx-bg-color:#F7F7FF;"><semantics><mrow><mn mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.813</mn><mo mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">±</mo><mtext class="ltx_mathvariant_bold" mathbackground="#F7F7FF" style="--ltx-bg-color:#F7F7FF;">0.212</mtext></mrow><annotation encoding="application/x-tex">0.813\pm\textbf{0.212}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Behavior or GRPO and ES in terms of mean conciseness reward and mean KL divergence. The label <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup> indicates cases where reward hacking was observed. Only models that did not hack the reward were included in the results.

</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">ES fine-tuning is reliable across runs.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px5.p1">
<p class="ltx_p">Fine-tuning LLMs is computationally expensive, and it is therefore critical that it leads to consistent results across runs. Table <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.T2" title="Table 2 ‣ ES is more robust against reward hacking. ‣ 4.2 Behavioral Differences between ES and RL in fine-tuning for conciseness ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">2</span></a> presents the mean and standard deviation of the conciseness reward and KL divergence across four independent runs after <math alttext="1,000" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p1.m1" intent=":literal"><semantics><mrow><mn>1</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">1,000</annotation></semantics></math> iterations. A mean reward cut-off of <math alttext="&gt;0.85" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p1.m2" intent=":literal"><semantics><mrow><mi></mi><mo>&gt;</mo><mn>0.85</mn></mrow><annotation encoding="application/x-tex">&gt;0.85</annotation></semantics></math> was used to down-select hyperparameter combinations, ensuring that only the best ES and GRPO configurations were included in the analysis.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px5.p2">
<p class="ltx_p">As shown in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.T2" title="Table 2 ‣ ES is more robust against reward hacking. ‣ 4.2 Behavioral Differences between ES and RL in fine-tuning for conciseness ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">2</span></a>, ES achieved consistent conciseness rewards,indicated by a low reward standard deviation (<math alttext="0.004" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p2.m1" intent=":literal"><semantics><mn>0.004</mn><annotation encoding="application/x-tex">0.004</annotation></semantics></math> and <math alttext="0.008" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p2.m2" intent=":literal"><semantics><mn>0.008</mn><annotation encoding="application/x-tex">0.008</annotation></semantics></math>) over four runs with different random seeds. GRPO has <math alttext="15.5\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS0.Px5.p2.m3" intent=":literal"><semantics><mrow><mn>15.5</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">15.5\times</annotation></semantics></math> higher standard deviation (<math alttext="0.041" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p2.m4" intent=":literal"><semantics><mn>0.041</mn><annotation encoding="application/x-tex">0.041</annotation></semantics></math>–<math alttext="0.062" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p2.m5" intent=":literal"><semantics><mn>0.062</mn><annotation encoding="application/x-tex">0.062</annotation></semantics></math>), suggesting that its results were much less consistent. The results on KL divergence were similar. For instance, while ES (<math alttext="\sigma=0.0015" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p2.m6" intent=":literal"><semantics><mrow><mi>σ</mi><mo>=</mo><mn>0.0015</mn></mrow><annotation encoding="application/x-tex">\sigma=0.0015</annotation></semantics></math>) and GRPO (<math alttext="\beta=0.0167" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p2.m7" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0.0167</mn></mrow><annotation encoding="application/x-tex">\beta=0.0167</annotation></semantics></math>) achieved similar mean rewards, GRPO exhibits a <math alttext="1.95\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS0.Px5.p2.m8" intent=":literal"><semantics><mrow><mn>1.95</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">1.95\times</annotation></semantics></math> higher KL divergence mean and <math alttext="3.83\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS0.Px5.p2.m9" intent=":literal"><semantics><mrow><mn>3.83</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">3.83\times</annotation></semantics></math> greater standard deviation. Similarly, while ES (<math alttext="\sigma=0.001" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p2.m10" intent=":literal"><semantics><mrow><mi>σ</mi><mo>=</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">\sigma=0.001</annotation></semantics></math>) achieves a slightly lower reward compared to GRPO (<math alttext="\beta=0.0167" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p2.m11" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0.0167</mn></mrow><annotation encoding="application/x-tex">\beta=0.0167</annotation></semantics></math>), GRPO (<math alttext="\beta=0.0167" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px5.p2.m12" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0.0167</mn></mrow><annotation encoding="application/x-tex">\beta=0.0167</annotation></semantics></math>) has a <math alttext="5.8\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS0.Px5.p2.m13" intent=":literal"><semantics><mrow><mn>5.8</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">5.8\times</annotation></semantics></math> higher KL divergence and a <math alttext="8.44\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS0.Px5.p2.m14" intent=":literal"><semantics><mrow><mn>8.44</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">8.44\times</annotation></semantics></math> higher standard deviation. Thus, ES fine-tuning is more reliable than GRPO.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p">Exploration in parameter space plays a key role in the surprisingly good fine-tuning performance of ES. As discussed by <cite class="ltx_cite ltx_citemacro_citet">Rückstieß et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib49" title="">2010</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Plappert et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib41" title="">2018</a>)</cite>, sampling noise in parameter space ensures that the entire action trajectory, i.e., the sequence of tokens, only depends on one single sampling, leading to significantly lower variance in rollouts, i.e., in response generation. As a result, gradient estimation is more reliable and convergence is more stable. In contrast, action space exploration in RL injects noise at every step, i.e., at each token position, resulting in high variance in the sequence generation. The behavior of RL therefore is much less reliable than ES, as was seen in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.T2" title="Table 2 ‣ ES is more robust against reward hacking. ‣ 4.2 Behavioral Differences between ES and RL in fine-tuning for conciseness ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">2</span></a>. Moreover, step-wise exploration in action space promotes reward hacking by increasing the chance of sampling a single hacking action. One example is the nonsensical symbol sampled during RL that can hack the conciseness reward.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p">Another key difference between ES and RL is that ES intrinsically optimizes a solution distribution <cite class="ltx_cite ltx_citemacro_citep">(Lehman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib30" title="">2018</a>)</cite>, while RL optimizes a single solution. This property makes it more difficult for ES to hack the reward since a single hacked solution usually does not have a high-quality solution distribution around it. This property also results in solutions that are more robust to noisy perturbations in parameter space <cite class="ltx_cite ltx_citemacro_citep">(Lehman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib30" title="">2018</a>)</cite>, making them more robust to adversarial attacks and less likely to be compromised in other follow-up fine-tuning tasks <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib7" title="">2025</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p">In the experiments in this paper, extensive hyperparameter tuning was performed for RL methods, resulting in specific RL hyperparameters for different model sizes and families. In comparison, ES was found to be less sensitive to hyperparameters, and the same set of hyperparameters was used for all experiments. While there are many common enhancements for ES <cite class="ltx_cite ltx_citemacro_citep">(Salimans et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>)</cite>, none were used in the experiments so that the power of vanilla ES could be clearly demonstrated. Thus, it may be possible to improve the results with more extensive hyperparameter tuning and other enhancements.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p">One counterintuitive result is that the ES implementation only needs a population of 30 to effectively optimize billions of parameters. In contrast, previous work <cite class="ltx_cite ltx_citemacro_citep">(Salimans et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib51" title="">2017</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib76" title="">2017</a>; Lehman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib30" title="">2018</a>; Lorenc &amp; Neruda, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib34" title="">2025</a>)</cite> used populations of 10,000 or more for models with millions or fewer parameters. An interesting future direction is to analyze how such small populations are possible. Perhaps this is related to the observed low intrinsic dimensionality of LLMs <cite class="ltx_cite ltx_citemacro_citep">(Aghajanyan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib2" title="">2021</a>)</cite>. Another promising direction is to use ES to perform unsupervised fine-tuning based on internal behaviors of LLMs, such as confidence calculated based on semantic entropy and semantic density <cite class="ltx_cite ltx_citemacro_citep">(Qiu &amp; Miikkulainen, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib42" title="">2024</a>; Farquhar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib13" title="">2024</a>)</cite>. Such fine-tuning cannot be done with RL, since action space exploration does not change the internal representations of LLMs (that is, each action sampling is generated via output distribution without changing the internal parameters). In a broader sense, since ES does not need process rewards during exploration, it may be a necessary ingredient for superintelligence <cite class="ltx_cite ltx_citemacro_citep">(Mucci &amp; Stryker, <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib37" title="">2023</a>)</cite>, which would be difficult to achieve by supervised learning using process guidance from human data. Massive parallelization of ES will speed up exploration by distributing the computations across GPU machines or even data centers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p">An important question is: what are the underlying computational mechanisms that make ES and RL behave so differently? While this question requires significant further work, a possible hypothesis emerges from the experiments in this paper. Many fine-tuning objectives, like conciseness and the Countdown task, are long-horizon outcome-only objectives. The reward signal is jagged, making it difficult to navigate with gradient-based post-training methods. RL and ES both provide workarounds via effective noise injection to “smooth out” the jagged reward landscape. In the case of RL, noise is introduced from Monte-Carlo sampling of each token during a rollout, averaged over many rollouts, which effectively smooths the sampling process but does not necessarily guarantee that the reward landscape is smooth in parameter space. RL’s gradient estimation therefore has a high-variance, and its signal-to-noise ratio becomes worse with longer sequences and sharper policies (i.e. those with lower entropy), and therefore prone to undesirable outcomes such as reward hacking.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.p6">
<p class="ltx_p">In contrast, ES injects noise directly into the parameter space via explicit Gaussian convolution, which effectively smooths out the jagged reward landscape. As a result, it provides a more stable way of exploring the landscape, leading to more consistent, efficient, and robust optimization (as observed in the experiments and in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS5" title="A.5 Parameter Magnitude Shifts by Evolutionary fine-tuning ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">A.5</span></a>). Moreover, the larger the models and the sharper the policies, the more jagged the reward landscapes; therefore, ES is likely to have an advantage in fine-tuning them. Direct evidence for this hypothesis still needs to be obtained, but it provides a plausible mechanistic explanation, and a direction for future work. Eventually such work could result in better fine-tuning methods, as well as an improved understanding of LLMs in general.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p">This paper introduces and evaluates a new approach to fine-tuning LLMs, based on scaling up ES to billions of dimensions. The approached performed significantly better than the standard RL fine-tuning in the Countdown task, which has sparse long-horizon rewards. ES was found to be more sample efficient, less sensitive to hyperparameter setup, and to achieve consistently better results across multiple LLMs. Further empirical studies on fine-tuning for conciseness revealed that ES is less likely to hack the reward, and behaves reliably across multiple runs. The mechanisms underlying these differences still need to be characterized, but a plausible hypothesis is that the exploration in ES is better suited for the jagged reward landscapes in large models. ES therefore constitutes a promising alternative to RL in fine-tuning LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S6.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Reproducibility statement</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.SS0.SSSx1.p1">
<p class="ltx_p">The experimental and parametric setup is provided in full details in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS1" title="A.1 Experimental Setup ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">A.1</span></a> for reproducing all the experimental results reported in this paper. Source codes for generating the experimental results are provided at: <a class="ltx_ref ltx_href" href="https://github.com/VsonicV/es-fine-tuning-paper" title="">https://github.com/VsonicV/es-fine-tuning-paper</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Josh Achiam et&nbsp;al.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2303.08774</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aghajanyan et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Intrinsic dimensionality explains the effectiveness of language model fine-tuning.

</span>
<span class="ltx_bibblock">In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pp.  7319–7328, Online, August 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.acl-long.568</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-long.568/" title="">https://aclanthology.org/2021.acl-long.568/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI@Meta (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
AI@Meta.

</span>
<span class="ltx_bibblock">Llama 3 model card, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" title="">https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akiba et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Takuya Akiba, Makoto Shing, Yujin Tang, Qi&nbsp;Sun, and David Ha.

</span>
<span class="ltx_bibblock">Evolutionary optimization of model merging recipes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, 7(2):195–204, 2025.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s42256-024-00975-8</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s42256-024-00975-8" title="">https://doi.org/10.1038/s42256-024-00975-8</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing Claude 4, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/claude-4" title="">https://www.anthropic.com/news/claude-4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with reinforcement learning from human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2204.05862</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.05862" title="">https://arxiv.org/abs/2204.05862</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Huanran Chen, Yinpeng Dong, Zeming Wei, Yao Huang, Yichi Zhang, Hang Su, and Jun Zhu.

</span>
<span class="ltx_bibblock">Understanding pre-training and fine-tuning from loss landscape perspectives.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2505.17646</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2505.17646" title="">https://arxiv.org/abs/2505.17646</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choshen et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend.

</span>
<span class="ltx_bibblock">On the weaknesses of reinforcement learning for neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=H1eCw3EKvH" title="">https://openreview.net/forum?id=H1eCw3EKvH</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chrabaszcz et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.

</span>
<span class="ltx_bibblock">Back to basics: benchmarking canonical evolution strategies for playing atari.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 27th International Joint Conference on Artificial Intelligence</em>, IJCAI’18, pp.  1419–1426. AAAI Press, 2018.

</span>
<span class="ltx_bibblock">ISBN 9780999241127.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conti et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edoardo Conti, Vashisht Madhavan, Felipe Petroski&nbsp;Such, Joel Lehman, Kenneth Stanley, and Jeff Clune.

</span>
<span class="ltx_bibblock">Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents.

</span>
<span class="ltx_bibblock">In S.&nbsp;Bengio, H.&nbsp;Wallach, H.&nbsp;Larochelle, K.&nbsp;Grauman, N.&nbsp;Cesa-Bianchi, and R.&nbsp;Garnett (eds.), <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume&nbsp;31. Curran Associates, Inc., 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denison et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel&nbsp;R. Bowman, Ethan Perez, and Evan Hubinger.

</span>
<span class="ltx_bibblock">Sycophancy to subterfuge: Investigating reward-tampering in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2406.10162</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.10162" title="">https://arxiv.org/abs/2406.10162</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guodong Du, Jing Li, Hanting Liu, Runhua Jiang, Shuyang Yu, Yifei Guo, Sim&nbsp;Kuan Goh, and Ho-Kin Tang.

</span>
<span class="ltx_bibblock">Knowledge fusion by evolving weights of language models.

</span>
<span class="ltx_bibblock">In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), <em class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2024</em>, pp.  11727–11742, Bangkok, Thailand, August 2024. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2024.findings-acl.698</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.findings-acl.698/" title="">https://aclanthology.org/2024.findings-acl.698/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farquhar et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal.

</span>
<span class="ltx_bibblock">Detecting hallucinations in large language models using semantic entropy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature</em>, 630(8017):625–630, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi&nbsp;Han, and Yanghua Xiao.

</span>
<span class="ltx_bibblock">Reward shaping to mitigate reward hacking in RLHF.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2502.18770</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2502.18770" title="">https://arxiv.org/abs/2502.18770</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandhi et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kanishk Gandhi, Ayush&nbsp;K Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman.

</span>
<span class="ltx_bibblock">Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective STars.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Second Conference on Language Modeling</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=QGJ9ttXLTy" title="">https://openreview.net/forum?id=QGJ9ttXLTy</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, John Schulman, and Jacob Hilton.

</span>
<span class="ltx_bibblock">Scaling laws for reward model overoptimization.

</span>
<span class="ltx_bibblock">In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, volume 202 of <em class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pp.  10835–10866. PMLR, 23–29 Jul 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v202/gao23h.html" title="">https://proceedings.mlr.press/v202/gao23h.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Deep learning</em>, volume&nbsp;1.

</span>
<span class="ltx_bibblock">MIT Press, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities., 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf" title="">https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. (2025a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daya Guo et&nbsp;al.

</span>
<span class="ltx_bibblock">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2501.12948</em>, 2025a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2501.12948" title="">https://arxiv.org/abs/2501.12948</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. (2025b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daya Guo et&nbsp;al.

</span>
<span class="ltx_bibblock">Deepseek-r1 incentivizes reasoning in llms through reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature</em>, 645(8081):633–638, 2025b.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41586-025-09422-z</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41586-025-09422-z" title="">https://doi.org/10.1038/s41586-025-09422-z</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu&nbsp;Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang.

</span>
<span class="ltx_bibblock">Connecting large language models with evolutionary algorithms yields powerful prompt optimizers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=ZG3RaNIsO8" title="">https://openreview.net/forum?id=ZG3RaNIsO8</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hansen &amp; Ostermeier (2001)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nikolaus Hansen and Andreas Ostermeier.

</span>
<span class="ltx_bibblock">Completely derandomized self-adaptation in evolution strategies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Evolutionary Computation</em>, 9(2):159–195, 2001.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1162/106365601750190398</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bo&nbsp;Huang, Yuxin Jiang, Mingyang Chen, Yi&nbsp;Wang, Hongyang Chen, and Wei Wang.

</span>
<span class="ltx_bibblock">When evolution strategy meets language models tuning.

</span>
<span class="ltx_bibblock">In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara&nbsp;Di Eugenio, and Steven Schockaert (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the 31st International Conference on Computational Linguistics</em>, pp.  5333–5344, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2025.coling-main.357/" title="">https://aclanthology.org/2025.coling-main.357/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Igel (2003)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christian Igel.

</span>
<span class="ltx_bibblock">Neuroevolution for reinforcement learning using evolution strategies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2003 Congress on Evolutionary Computation</em>, pp.  2588–2595, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zeyu Jia, Alexander Rakhlin, and Tengyang Xie.

</span>
<span class="ltx_bibblock">Do we need to verify step by step? rethinking process supervision from a theoretical perspective.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Forty-second International Conference on Machine Learning</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=4BfaPHfhJ0" title="">https://openreview.net/forum?id=4BfaPHfhJ0</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Emma&nbsp;Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio&nbsp;Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven&nbsp;Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William&nbsp;El Sayed.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2401.04088</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Feihu Jin, Yifan Liu, and Ying Tan.

</span>
<span class="ltx_bibblock">Derivative-free optimization for low-rank adaptation in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">IEEE/ACM Trans. Audio, Speech and Lang. Proc.</em>, 32:4607–4616, October 2024.

</span>
<span class="ltx_bibblock">ISSN 2329-9290.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TASLP.2024.3477330</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TASLP.2024.3477330" title="">https://doi.org/10.1109/TASLP.2024.3477330</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma &amp; Ba (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Diederik&nbsp;P. Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In Yoshua Bengio and Yann LeCun (eds.), <em class="ltx_emph ltx_font_italic">3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</em>, 2015.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1412.6980" title="">http://arxiv.org/abs/1412.6980</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latif &amp; Zhai (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ehsan Latif and Xiaoming Zhai.

</span>
<span class="ltx_bibblock">Fine-tuning chatgpt for automatic scoring.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Computers and Education: Artificial Intelligence</em>, 6:100210, 2024.

</span>
<span class="ltx_bibblock">ISSN 2666-920X.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.caeai.2024.100210</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S2666920X24000110" title="">https://www.sciencedirect.com/science/article/pii/S2666920X24000110</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lehman et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joel Lehman, Jay Chen, Jeff Clune, and Kenneth&nbsp;O. Stanley.

</span>
<span class="ltx_bibblock">Es is more than just a traditional finite-difference approximator.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Genetic and Evolutionary Computation Conference</em>, GECCO ’18, pp.  450–457, New York, NY, USA, 2018. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450356183.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3205455.3205474</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3205455.3205474" title="">https://doi.org/10.1145/3205455.3205474</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lehman et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth&nbsp;O. Stanley.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Evolution Through Large Models</em>, pp.  331–366.

</span>
<span class="ltx_bibblock">Springer Nature Singapore, Singapore, 2024.

</span>
<span class="ltx_bibblock">ISBN 978-981-99-3814-8.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-981-99-3814-8˙11</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-981-99-3814-8_11" title="">https://doi.org/10.1007/978-981-99-3814-8_11</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et&nbsp;al.

</span>
<span class="ltx_bibblock">Deepseek-v3 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2412.19437</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh, and Yang You.

</span>
<span class="ltx_bibblock">Sparse meZO: Less parameters for better performance in zeroth-order LLM fine-tuning, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=4Kw4KAoVnx" title="">https://openreview.net/forum?id=4Kw4KAoVnx</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lorenc &amp; Neruda (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Matyáš Lorenc and Roman Neruda.

</span>
<span class="ltx_bibblock">Utilizing evolution strategies to train transformers in reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2501.13883</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2501.13883" title="">https://arxiv.org/abs/2501.13883</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malladi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason&nbsp;D Lee, Danqi Chen, and Sanjeev Arora.

</span>
<span class="ltx_bibblock">Fine-tuning language models with just forward passes.

</span>
<span class="ltx_bibblock">In A.&nbsp;Oh, T.&nbsp;Naumann, A.&nbsp;Globerson, K.&nbsp;Saenko, M.&nbsp;Hardt, and S.&nbsp;Levine (eds.), <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume&nbsp;36, pp.  53038–53075. Curran Associates, Inc., 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/a627810151be4d13f907ac898ff7e948-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2023/file/a627810151be4d13f907ac898ff7e948-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meyerson et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elliot Meyerson, Mark&nbsp;J. Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy&nbsp;K. Hoover, and Joel Lehman.

</span>
<span class="ltx_bibblock">Language model crossover: Variation through few-shot prompting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ACM Trans. Evol. Learn. Optim.</em>, 4(4), November 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3694791</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3694791" title="">https://doi.org/10.1145/3694791</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mucci &amp; Stryker (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tim Mucci and Cole Stryker.

</span>
<span class="ltx_bibblock">What is artificial superintelligence?, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ibm.com/think/topics/artificial-superintelligence" title="">https://www.ibm.com/think/topics/artificial-superintelligence</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Novikov et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexander Novikov, Ngân Vũ, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam&nbsp;Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J.&nbsp;R. Ruiz, Abbas Mehrabian, M.&nbsp;Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, and Matej Balog.

</span>
<span class="ltx_bibblock">AlphaEvolve: A coding agent for scientific and algorithmic discovery.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2506.13131</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2506.13131" title="">https://arxiv.org/abs/2506.13131</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 36th International Conference on Neural Information Processing Systems</em>, NIPS ’22, Red Hook, NY, USA, 2022. Curran Associates Inc.

</span>
<span class="ltx_bibblock">ISBN 9781713871088.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr.

</span>
<span class="ltx_bibblock">Tinyzero.

</span>
<span class="ltx_bibblock">https://github.com/Jiayi-Pan/TinyZero, 2025.

</span>
<span class="ltx_bibblock">Accessed: 2025-01-24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plappert et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard&nbsp;Y. Chen, Xi&nbsp;Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz.

</span>
<span class="ltx_bibblock">Parameter space noise for exploration.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=ByBAl2eAZ" title="">https://openreview.net/forum?id=ByBAl2eAZ</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu &amp; Miikkulainen (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xin Qiu and Risto Miikkulainen.

</span>
<span class="ltx_bibblock">Semantic density: Uncertainty quantification for large language models through confidence measurement in semantic space.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 38th Conference on Neural Information Processing Systems</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher&nbsp;D. Manning, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: your language model is secretly a reward model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 37th International Conference on Neural Information Processing Systems</em>, NIPS ’23, Red Hook, NY, USA, 2023. Curran Associates Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rechenberg (1973)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Rechenberg.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution</em>.

</span>
<span class="ltx_bibblock">Problemata (Stuttgart). Frommann-Holzboog, 1973.

</span>
<span class="ltx_bibblock">ISBN 9783772803741.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://books.google.com/books?id=-WAQAQAAMAAJ" title="">https://books.google.com/books?id=-WAQAQAAMAAJ</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Risi &amp; Stanley (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sebastian Risi and Kenneth&nbsp;O. Stanley.

</span>
<span class="ltx_bibblock">Deep neuroevolution of recurrent and discrete world models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Genetic and Evolutionary Computation Conference</em>, GECCO ’19, pp.  456–462, New York, NY, USA, 2019. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450361118.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3321707.3321817</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3321707.3321817" title="">https://doi.org/10.1145/3321707.3321817</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Romera-Paredes et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M.&nbsp;Pawan Kumar, Emilien Dupont, Francisco J.&nbsp;R. Ruiz, Jordan&nbsp;S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi.

</span>
<span class="ltx_bibblock">Mathematical discoveries from program search with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature</em>, 625(7995):468–475, 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41586-023-06924-6</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41586-023-06924-6" title="">https://doi.org/10.1038/s41586-023-06924-6</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rozière et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing&nbsp;Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian&nbsp;Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2308.12950</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rückstieß et&nbsp;al. (2008)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Thomas Rückstieß, Martin Felder, and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">State-dependent exploration for policy gradient methods.

</span>
<span class="ltx_bibblock">In Walter Daelemans, Bart Goethals, and Katharina Morik (eds.), <em class="ltx_emph ltx_font_italic">Machine Learning and Knowledge Discovery in Databases</em>, pp.  234–249, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg.

</span>
<span class="ltx_bibblock">ISBN 978-3-540-87481-2.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rückstieß et&nbsp;al. (2010)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Thomas Rückstieß, Frank Sehnke, Tom Schaul, Daan Wierstra, Yi&nbsp;Sun, and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Exploring parameter space in reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Paladyn</em>, 1(1):14–24, 2010.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.2478/s13230-010-0002-4</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.2478/s13230-010-0002-4" title="">https://doi.org/10.2478/s13230-010-0002-4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salimans et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi&nbsp;Chen.

</span>
<span class="ltx_bibblock">Improved techniques for training gans.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 30th International Conference on Neural Information Processing Systems</em>, NIPS’16, pp.  2234–2242, Red Hook, NY, USA, 2016. Curran Associates Inc.

</span>
<span class="ltx_bibblock">ISBN 9781510838819.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salimans et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tim Salimans, Jonathan Ho, Xi&nbsp;Chen, Szymon Sidor, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Evolution strategies as a scalable alternative to reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:1703.03864</em>, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1703.03864" title="">https://arxiv.org/abs/1703.03864</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanchez&nbsp;Carmona et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vicente&nbsp;Ivan Sanchez&nbsp;Carmona, Shanshan Jiang, and Bin Dong.

</span>
<span class="ltx_bibblock">How well can a genetic algorithm fine-tune transformer encoders? a first approach.

</span>
<span class="ltx_bibblock">In Shabnam Tafreshi, Arjun Akula, João Sedoc, Aleksandr Drozd, Anna Rogers, and Anna Rumshisky (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the Fifth Workshop on Insights from Negative Results in NLP</em>, pp.  25–33, Mexico City, Mexico, June 2024. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2024.insights-1.4</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.insights-1.4/" title="">https://aclanthology.org/2024.insights-1.4/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
John Schulman.

</span>
<span class="ltx_bibblock">Approximating kl divergence, 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">URL http://joschu. net/blog/kl-approx. html</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.

</span>
<span class="ltx_bibblock">Proximal policy optimization algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:1707.06347</em>, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1707.06347" title="">https://arxiv.org/abs/1707.06347</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwefel (1977)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hans-Paul Schwefel.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Numerische Optimierung von Computermodellen mittels der Evo-lutionsstrategie</em>, volume&nbsp;26.

</span>
<span class="ltx_bibblock">01 1977.

</span>
<span class="ltx_bibblock">ISBN 9783764308766.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-0348-5927-1</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sehnke et&nbsp;al. (2010)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Frank Sehnke, Christian Osendorfer, Thomas Rückstieß, Alex Graves, Jan Peters, and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Parameter-exploring policy gradients.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Neural Networks</em>, 23(4):551–559, 2010.

</span>
<span class="ltx_bibblock">ISSN 0893-6080.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.neunet.2009.12.004</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0893608009003220" title="">https://www.sciencedirect.com/science/article/pii/S0893608009003220</a>.

</span>
<span class="ltx_bibblock">The 18th International Conference on Artificial Neural Networks, ICANN 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.&nbsp;K. Li, Y.&nbsp;Wu, and Daya Guo.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2402.03300</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.03300" title="">https://arxiv.org/abs/2402.03300</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le&nbsp;Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise&nbsp;Aguera y&nbsp;Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S.&nbsp;Sara Mahdavi, Joelle Barral, Dale Webster, Greg&nbsp;S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan.

</span>
<span class="ltx_bibblock">Towards expert-level medical question answering with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2305.09617</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu&nbsp;Cheng.

</span>
<span class="ltx_bibblock">PRMBench: A fine-grained and challenging benchmark for process-level reward models.

</span>
<span class="ltx_bibblock">In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad&nbsp;Taher Pilehvar (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  25299–25346, Vienna, Austria, July 2025. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">ISBN 979-8-89176-251-0.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2025.acl-long.1230</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2025.acl-long.1230/" title="">https://aclanthology.org/2025.acl-long.1230/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spall (1992)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.C. Spall.

</span>
<span class="ltx_bibblock">Multivariate stochastic approximation using a simultaneous perturbation gradient approximation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">IEEE Transactions on Automatic Control</em>, 37(3):332–341, 1992.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/9.119632</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava &amp; Aggarwal (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Saksham&nbsp;Sahai Srivastava and Vaneet Aggarwal.

</span>
<span class="ltx_bibblock">A technical survey of reinforcement learning techniques for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2507.04136</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2507.04136" title="">https://arxiv.org/abs/2507.04136</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Such et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Felipe&nbsp;Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth&nbsp;O. Stanley, and Jeff Clune.

</span>
<span class="ltx_bibblock">Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:1712.06567</em>, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:5044808" title="">https://api.semanticscholar.org/CorpusID:5044808</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">BBTv2: Towards a gradient-free future with large language models.

</span>
<span class="ltx_bibblock">In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pp.  3916–3930, Abu Dhabi, United Arab Emirates, December 2022a. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.emnlp-main.259</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-main.259/" title="">https://aclanthology.org/2022.emnlp-main.259/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">Black-box tuning for language-model-as-a-service.

</span>
<span class="ltx_bibblock">In Kamalika Chaudhuri, Stefanie Jegelka, Le&nbsp;Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the 39th International Conference on Machine Learning</em>, volume 162 of <em class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pp.  20841–20855. PMLR, 17–23 Jul 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v162/sun22e.html" title="">https://proceedings.mlr.press/v162/sun22e.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutton &amp; Barto (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Richard&nbsp;S. Sutton and Andrew&nbsp;G. Barto.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Reinforcement Learning: An Introduction</em>.

</span>
<span class="ltx_bibblock">MIT Press, Cambridge, MA, 2nd edition, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toledano-López et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Orlando&nbsp;Grabiel Toledano-López, Julio Madera, Hector González, Alfredo Simón-Cuevas, Thomas Demeester, and Erik Mannens.

</span>
<span class="ltx_bibblock">Fine-tuning mt5-based transformer via cma-es for sentiment analysis.

</span>
<span class="ltx_bibblock">In Manuel&nbsp;Montes y&nbsp;Gómez, Julio Gonzalo, Francisco Rangel, Marco Casavantes, Miguel Ángel&nbsp;Álvare Carmona, Gemma&nbsp;Bel Enguix, Hugo&nbsp;Jair Escalante, Larissa&nbsp;A. de&nbsp;Freitas, Antonio Miranda-Escalada, Francisco&nbsp;J. Rodríguez-Sánchez, Aiala Rosá, Marco Antonio&nbsp;Sobrevilla Cabezudo, Mariona Taulé, and Rafael Valencia-García (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2022) co-located with the Conference of the Spanish Society for Natural Language Processing (SEPLN 2022), A Coruña, Spain, September 20, 2022</em>, volume 3202 of <em class="ltx_emph ltx_font_italic">CEUR Workshop Proceedings</em>. CEUR-WS.org, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ceur-ws.org/Vol-3202/restmex-paper12.pdf" title="">http://ceur-ws.org/Vol-3202/restmex-paper12.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Uesato et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins.

</span>
<span class="ltx_bibblock">Solving math word problems with process- and outcome-based feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2211.14275</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.14275" title="">https://arxiv.org/abs/2211.14275</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vemula et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anirudh Vemula, Wen Sun, and J.&nbsp;Andrew Bagnell.

</span>
<span class="ltx_bibblock">Contrasting exploration in parameter and action space: A zeroth order optimization perspective.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of 22nd International Conference on Artificial Intelligence and Statistics (AISTATS ’19)</em>, March 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chao Wang, Jiaxuan Zhao, Licheng Jiao, Lingling Li, Fang Liu, and Shuyuan Yang.

</span>
<span class="ltx_bibblock">When large language models meet evolutionary algorithms: Potential enhancements and challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Research</em>, 8:0646, 2025.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.34133/research.0646</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://spj.science.org/doi/abs/10.34133/research.0646" title="">https://spj.science.org/doi/abs/10.34133/research.0646</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wierstra et&nbsp;al. (2008)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber.

</span>
<span class="ltx_bibblock">Natural evolution strategies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)</em>, pp.  3381–3387, 2008.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CEC.2008.4631255</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wierstra et&nbsp;al. (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi&nbsp;Sun, Jan Peters, and JÃ¼rgen Schmidhuber.

</span>
<span class="ltx_bibblock">Natural evolution strategies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 15(27):949–980, 2014.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v15/wierstra14a.html" title="">http://jmlr.org/papers/v15/wierstra14a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.

</span>
<span class="ltx_bibblock">Bloomberggpt: A large language model for finance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2303.17564</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xingyu Wu, Sheng-Hao Wu, Jibin Wu, Liang Feng, and Kay&nbsp;Chen Tan.

</span>
<span class="ltx_bibblock">Evolutionary computation in the era of large language model: Survey and roadmap.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">IEEE Transactions on Evolutionary Computation</em>, 29(2):534–554, 2025.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TEVC.2024.3506731</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
An&nbsp;Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen2. 5-1m technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2501.15383</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xingwen Zhang, Jeff Clune, and Kenneth&nbsp;O. Stanley.

</span>
<span class="ltx_bibblock">On the relationship between the openai evolution strategy and stochastic gradient descent.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:1712.06564</em>, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1712.06564" title="">https://arxiv.org/abs/1712.06564</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin.

</span>
<span class="ltx_bibblock">The lessons of developing process reward models in mathematical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:2501.07301</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2501.07301" title="">https://arxiv.org/abs/2501.07301</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiangjiang Zhao, Zhuoran Wang, and Fangchun Yang.

</span>
<span class="ltx_bibblock">Genetic prompt search via exploiting language model probabilities.

</span>
<span class="ltx_bibblock">In Edith Elkind (ed.), <em class="ltx_emph ltx_font_italic">Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23</em>, pp.  5296–5305. International Joint Conferences on Artificial Intelligence Organization, 8 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.24963/ijcai.2023/588</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.24963/ijcai.2023/588" title="">https://doi.org/10.24963/ijcai.2023/588</a>.

</span>
<span class="ltx_bibblock">Main Track.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Han Zhong, Zikang Shan, Guhao Feng, Wei Xiong, Xinle Cheng, Li&nbsp;Zhao, Di&nbsp;He, Jiang Bian, and Liwei Wang.

</span>
<span class="ltx_bibblock">DPO meets PPO: Reinforced token optimization for RLHF.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Forty-second International Conference on Machine Learning</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=IfWKVF6LfY" title="">https://openreview.net/forum?id=IfWKVF6LfY</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Experimental Setup</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Experimental setup for the Countdown experiments.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p">Representative models from the Qwen2.5 family (0.5B–7B) and the LLaMA3 family (1B–8B) were fine-tuned for this task. For the PPO experiments, a grid search was first performed around common hyperparameter settings and the best-performing values used (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.T3" title="Table 3 ‣ Experimental setup for the Countdown experiments. ‣ A.1 Experimental Setup ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">3</span></a>). For the GRPO experiments, a grid search was performed around the settings of <cite class="ltx_cite ltx_citemacro_cite">Pan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib40" title="">2025</a>)</cite> and the best-performing values used. GRPO experiments were run with two different group sizes: <math alttext="N=8" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">N=8</annotation></semantics></math>, following the common practice in GRPO training for the Countdown task, and <math alttext="N=30" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.m2" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">N=30</annotation></semantics></math>, aligning with the population size in ES. For all the ES, GRPO and PPO, the total number of sample evaluations was the same. The ES population size was <math alttext="N=30" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.m3" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">N=30</annotation></semantics></math>, noise scale <math alttext="\sigma=0.001" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.m4" intent=":literal"><semantics><mrow><mi>σ</mi><mo>=</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">\sigma=0.001</annotation></semantics></math>, and learning rate <math alttext="\alpha=5\times 10^{-4}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.m5" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\alpha=5\times 10^{-4}</annotation></semantics></math> across all experiments. To evaluate accuracy, a set of 200 samples were used during training, and a different set of 2000 samples during testing. For ES, results were reported on the test set after training for 500 iterations. For RL, the training was stopped after the same total number of sample evaluations as in the ES runs. An example of the prompt and the response is provided in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS2" title="A.2 Examples of Countdown Task Outputs ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Method</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">(<math alttext="1\mathrm{e}{-3}" class="ltx_Math" display="inline" id="A1.T3.m1" intent=":literal"><semantics><mrow><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">e</mi></mrow><mo>−</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">1\mathrm{e}{-3}</annotation></semantics></math>, <math alttext="1\mathrm{e}{-6}" class="ltx_Math" display="inline" id="A1.T3.m2" intent=":literal"><semantics><mrow><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">e</mi></mrow><mo>−</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">1\mathrm{e}{-6}</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">(<math alttext="1\mathrm{e}{-3}" class="ltx_Math" display="inline" id="A1.T3.m3" intent=":literal"><semantics><mrow><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">e</mi></mrow><mo>−</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">1\mathrm{e}{-3}</annotation></semantics></math>, <math alttext="1\mathrm{e}{-5}" class="ltx_Math" display="inline" id="A1.T3.m4" intent=":literal"><semantics><mrow><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">e</mi></mrow><mo>−</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">1\mathrm{e}{-5}</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">(<math alttext="5\mathrm{e}{-3}" class="ltx_Math" display="inline" id="A1.T3.m5" intent=":literal"><semantics><mrow><mrow><mn>5</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">e</mi></mrow><mo>−</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">5\mathrm{e}{-3}</annotation></semantics></math>, <math alttext="1\mathrm{e}{-6}" class="ltx_Math" display="inline" id="A1.T3.m6" intent=":literal"><semantics><mrow><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">e</mi></mrow><mo>−</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">1\mathrm{e}{-6}</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">(<math alttext="5\mathrm{e}{-3}" class="ltx_Math" display="inline" id="A1.T3.m7" intent=":literal"><semantics><mrow><mrow><mn>5</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">e</mi></mrow><mo>−</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">5\mathrm{e}{-3}</annotation></semantics></math>, <math alttext="1\mathrm{e}{-5}" class="ltx_Math" display="inline" id="A1.T3.m8" intent=":literal"><semantics><mrow><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">e</mi></mrow><mo>−</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">1\mathrm{e}{-5}</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" rowspan="7" style="padding-left:4.0pt;padding-right:4.0pt;">PPO</td>
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Qwen-0.5B-Instruct</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">✓</span></td>
<td class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Qwen-1.5B-Instruct</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Qwen-3B-Instruct</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Qwen-7B-Instruct</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-1B-Instruct</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-3B-Instruct</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-8B-Instruct</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="7" style="padding-left:4.0pt;padding-right:4.0pt;">GRPO</td>
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Qwen-0.5B-Instruct</td>
<td class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Qwen-1.5B-Instruct</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Qwen-3B-Instruct</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Qwen-7B-Instruct</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-1B-Instruct</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-3B-Instruct</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-8B-Instruct</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Hyperparameter Sweep across Models under PPO and GRPO.
Each pair <math alttext="(\cdot,\cdot)" class="ltx_Math" display="inline" id="A1.T3.m13" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\cdot,\cdot)</annotation></semantics></math> denotes (KL-divergence penalty coefficient <math alttext="\beta" class="ltx_Math" display="inline" id="A1.T3.m14" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, learning rate <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.T3.m15" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>);
the label ’<math alttext="\checkmark" class="ltx_Math" display="inline" id="A1.T3.m16" intent=":literal"><semantics><mi mathvariant="normal">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math>’ indicates the best hyperparameter setting for each model-method combination.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Experimental setup for the Conciseness experiments.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p">In each experiment, Qwen-2.5-7B-Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib75" title="">2025</a>)</cite> was fine-tuned using both ES and GRPO and evaluated using a held-out evaluation set. Each run was repeated four times, using a different random seed each time.
For each GRPO experiment, the group size <math alttext="N=30" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m1" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">N=30</annotation></semantics></math>, and learning rate <math alttext="\alpha=5\times 10^{-6}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m2" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\alpha=5\times 10^{-6}</annotation></semantics></math>. Ten log-spaced values from <math alttext="0.01" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m3" intent=":literal"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation></semantics></math> to <math alttext="1.0" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m4" intent=":literal"><semantics><mn>1.0</mn><annotation encoding="application/x-tex">1.0</annotation></semantics></math> were evaluated for the the KL-divergence penalty coefficient <math alttext="\beta" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m5" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, as well as <math alttext="\beta=0.0" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m6" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0.0</mn></mrow><annotation encoding="application/x-tex">\beta=0.0</annotation></semantics></math>. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS3" title="A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">A.3</span></a> presents additional experiments with varying <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m7" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m8" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> values.
For ES, the population size <math alttext="N=30" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m9" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">N=30</annotation></semantics></math>, ensuring that GRPO and ES generated the same number of responses per prompt, resulting in the same training exposure. Models were fine-tuned with <math alttext="\sigma=\{0.0005,0.001,0.0015\}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m10" intent=":literal"><semantics><mrow><mi>σ</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>0.0005</mn><mo>,</mo><mn>0.001</mn><mo>,</mo><mn>0.0015</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma=\{0.0005,0.001,0.0015\}</annotation></semantics></math>, with a learning rate <math alttext="\alpha=\frac{\sigma}{2}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m11" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mfrac><mi>σ</mi><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\alpha=\frac{\sigma}{2}</annotation></semantics></math>.
Both GRPO and ES experiments were run for <math alttext="1,000" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m12" intent=":literal"><semantics><mrow><mn>1</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">1,000</annotation></semantics></math> iterations, and a checkpoint saved every <math alttext="200" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.m13" intent=":literal"><semantics><mn>200</mn><annotation encoding="application/x-tex">200</annotation></semantics></math> iterations. Table <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.T4" title="Table 4 ‣ A.2 Examples of Countdown Task Outputs ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">4</span></a> shows the dataset of prompts and verifiable solutions used during fine-tuning; note that it consists of only two examples. Similarly, Table <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.T5" title="Table 5 ‣ A.2 Examples of Countdown Task Outputs ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">5</span></a> lists the prompts and verifiable solutions used in evaluating each fine-tuned model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Examples of Countdown Task Outputs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.SS2" title="A.2 Examples of Countdown Task Outputs ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">A.2</span></a> shows an example of the Qwen2.5-3B model’s output on the Countdown task after 400 ES iterations. In order to improve accuracy, the model learned to adjust its formulas by comparing the generated answer with the correct one, and to verify its own answers for correctness.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Prompt</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Verifiable Solution</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Solve: <math alttext="3+5=" class="ltx_Math" display="inline" id="A1.T4.m1" intent=":literal"><semantics><mrow><mrow><mn>3</mn><mo>+</mo><mn>5</mn></mrow><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">3+5=</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math alttext="8" class="ltx_Math" display="inline" id="A1.T4.m2" intent=":literal"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">If all birds can fly and penguins are birds, can penguins fly</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">No</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Prompts and verifiable solutions used in fine-tuning the models for conciseness. Two examples is enough to achieve this goal.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Prompt</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Verifiable Solution</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">What is the capital of France?</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Paris</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Calculate: <math alttext="12\times 7=" class="ltx_Math" display="inline" id="A1.T5.m1" intent=":literal"><semantics><mrow><mrow><mn>12</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>7</mn></mrow><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">12\times 7=</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><math alttext="84" class="ltx_Math" display="inline" id="A1.T5.m2" intent=":literal"><semantics><mn>84</mn><annotation encoding="application/x-tex">84</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Is the statement “All cats are mammals” true or false?</td>
<td class="ltx_td ltx_align_center ltx_border_r">True</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r">What comes next in the sequence: <math alttext="2,4,6,8," class="ltx_Math" display="inline" id="A1.T5.m3" intent=":literal"><semantics><mrow><mrow><mn>2</mn><mo>,</mo><mn>4</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>8</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">2,4,6,8,</annotation></semantics></math> ?</td>
<td class="ltx_td ltx_align_center ltx_border_r"><math alttext="10" class="ltx_Math" display="inline" id="A1.T5.m4" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Translate “Hello” to Spanish:</td>
<td class="ltx_td ltx_align_center ltx_border_r">Hola</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r">What is <math alttext="15\%" class="ltx_Math" display="inline" id="A1.T5.m5" intent=":literal"><semantics><mrow><mn>15</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">15\%</annotation></semantics></math> of <math alttext="200" class="ltx_Math" display="inline" id="A1.T5.m6" intent=":literal"><semantics><mn>200</mn><annotation encoding="application/x-tex">200</annotation></semantics></math>?</td>
<td class="ltx_td ltx_align_center ltx_border_r"><math alttext="30" class="ltx_Math" display="inline" id="A1.T5.m7" intent=":literal"><semantics><mn>30</mn><annotation encoding="application/x-tex">30</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Name one primary color:</td>
<td class="ltx_td ltx_align_center ltx_border_r">Red</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">How many days are in a week?</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><math alttext="7" class="ltx_Math" display="inline" id="A1.T5.m8" intent=":literal"><semantics><mn>7</mn><annotation encoding="application/x-tex">7</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Prompts and verifiable solutions used to evaluate the fine-tuned models. More examples are necessary than during fine-tuning to make the evaluation reliable.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS2.p2">
<span class="ltx_inline-block"><svg class="ltx_picture" height="1038.1" id="A1.SS2.p2.pic1" overflow="visible" version="1.1" viewBox="0 0 550 1038.1" width="550"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,1038.1) matrix(1 0 0 -1 0 0)"><clippath id="pgfcp13"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 0 L 0 1038.1 L 550 1038.1 L 550 0 Z"></path></clippath><g fill-rule="evenodd"><g fill="#B3B3B3" fill-opacity="1.0" style="--ltx-fill-color:#B3B3B3;"><path d="M 0 0 L 0 1038.1 L 550 1038.1 L 550 0 Z M 1.11 1.11 L 1.11 1036.99 L 548.89 1036.99 L 548.89 1.11 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0" style="--ltx-fill-color:#FFFFFF;"><path d="M 1.11 1.11 L 1.11 1036.99 L 548.89 1036.99 L 548.89 1.11 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.35 314.99)"><foreignobject color="#000000" height="1005.87" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :37.82em;--fo_height:51.09em;--fo_depth :21.6em;" transform="matrix(1 0 0 -1 0 706.99)" width="523.31"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<div class="ltx_block ltx_minipage ltx_align_bottom" style="width:37.82em;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#B3B3FF;">Iteration 400</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6E6FF;">[1] Input:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p">You are a helpful assistant. You first think about the reasoning process in your mind and then provide the user with the answer. Using the numbers [28 3 4 52], create an equation that equals 44. You can use basic arithmetic operations (<code class="ltx_verbatim ltx_font_typewriter">+</code>, <code class="ltx_verbatim ltx_font_typewriter">-</code>, <code class="ltx_verbatim ltx_font_typewriter">*</code>, <code class="ltx_verbatim ltx_font_typewriter">/</code>) and each number can only be used once. Show your work in <code class="ltx_verbatim ltx_font_typewriter">&lt;think&gt;</code> … <code class="ltx_verbatim ltx_font_typewriter">&lt;/think&gt;</code> tags. And return the final answer in <code class="ltx_verbatim ltx_font_typewriter">&lt;answer&gt;</code> … <code class="ltx_verbatim ltx_font_typewriter">&lt;/answer&gt;</code> tags, for example <code class="ltx_verbatim ltx_font_typewriter">&lt;answer&gt; (1 + 2) / 3 &lt;/answer&gt;</code>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6E6FF;">[1] Response:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p"><code class="ltx_verbatim ltx_font_typewriter">&lt;think&gt;</code> First, let’s consider all possible combinations of the numbers 49, 41, and 73 using basic arithmetic operations. We need to use each number exactly once and combine them to get 17.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p">1. Start with simple addition and subtraction:
- 49 + 41 - 73 = 80 - 73 = 7 (too high)
- 49 - 41 + 73 = 8 + 73 = 81 (too high)
- 73 - 49 - 41 = 24 - 41 = -17 (too low)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p">2. Try multiplication and division:
- (49 * 41) / 73 = 2029 / 73 <math alttext="\approx" class="ltx_Math" display="inline" id="A1.SS2.p2.pic1.m1" intent=":literal"><semantics><mo>≈</mo><annotation encoding="application/x-tex">\approx</annotation></semantics></math> 27.83 (not close to 17)
- (73 * 41) / 49 = 3013 / 49 <math alttext="\approx" class="ltx_Math" display="inline" id="A1.SS2.p2.pic1.m2" intent=":literal"><semantics><mo>≈</mo><annotation encoding="application/x-tex">\approx</annotation></semantics></math> 61.47 (too high)
- (49 * 73) / 41 = 3637 / 41 <math alttext="\approx" class="ltx_Math" display="inline" id="A1.SS2.p2.pic1.m3" intent=":literal"><semantics><mo>≈</mo><annotation encoding="application/x-tex">\approx</annotation></semantics></math> 88.80 (too high)
- (41 * 73) / 49 = 3013 / 49 <math alttext="\approx" class="ltx_Math" display="inline" id="A1.SS2.p2.pic1.m4" intent=":literal"><semantics><mo>≈</mo><annotation encoding="application/x-tex">\approx</annotation></semantics></math> 61.47 (same as above)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p">3. Check different orders:
- 73 - (49 + 41) = 73 - 90 = -17 (too low)
- 49 - (73 - 41) = 49 - 32 = 17 (this looks promising)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p">So, the equation 49 - (73 - 41) equals 17. Let’s verify it:
- Calculate inside the parentheses first: 73 - 41 = 32
- Then subtract from 49: 49 - 32 = 17</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p">This satisfies our requirement. <code class="ltx_verbatim ltx_font_typewriter">&lt;/think&gt;</code>
<code class="ltx_verbatim ltx_font_typewriter">&lt;answer&gt;</code> 49 - (73 - 41) <code class="ltx_verbatim ltx_font_typewriter">&lt;/answer&gt;</code></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p">Reward: 1.1000 (Format: 1.0000, Answer: 1.0000)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_block">Figure 2: </span>An example of a countdown task interaction.</figcaption>
</div></span></span></foreignobject></g></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Extended Conciseness Experiments</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p">In this section, the conciseness experiments are extended to investigate the impact of different learning rates on GRPO training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A1.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="498" id="A1.F3.sf1.g1" src="./es_files/x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>GRPO models results over various learning rates.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="498" id="A1.F3.sf2.g1" src="./es_files/x3.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>ES and GRPO Pareto fronts.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>GRPO behavior with different learning rates. (a) GRPO models trained using different learning rates and <math alttext="\beta" class="ltx_Math" display="inline" id="A1.F3.m2" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> values. Both conciseness reward and KL divergence increase with higher learning rates. (b) The ES Pareto front (blue line, top) plotted with the GRPO Pareto front (black line, bottom) over different model learning parameters. ES dominates GRPO across the whole range.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="A1.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">GRPO with different learning rates.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px1.p1">
<p class="ltx_p">Further GRPO experiments were run over four seeds with <math alttext="\beta=\{0,0.01,0.1,1.0\}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>0.01</mn><mo>,</mo><mn>0.1</mn><mo>,</mo><mn>1.0</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\beta=\{0,0.01,0.1,1.0\}</annotation></semantics></math>, varying the learning rate <math alttext="\alpha=\{2\times 10^{-6},3\times 10^{-6},4\times 10^{-6},5\times 10^{-6}\}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m2" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><mo>,</mo><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><mo>,</mo><mrow><mn>4</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><mo>,</mo><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha=\{2\times 10^{-6},3\times 10^{-6},4\times 10^{-6},5\times 10^{-6}\}</annotation></semantics></math>. A total of <math alttext="20" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m3" intent=":literal"><semantics><mn>20</mn><annotation encoding="application/x-tex">20</annotation></semantics></math> responses were sampled per evaluation prompt. Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F3.sf1" title="In Figure 3 ‣ A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">3(a)</span></a> shows the mean reward and KL divergence of each fine-tuned model. As the learning rate increases, both mean reward and mean KL divergence increase. The best models with respect to reward are trained using <math alttext="5\times 10^{-6}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m4" intent=":literal"><semantics><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\times 10^{-6}</annotation></semantics></math> and <math alttext="\beta=\{0.0,0.01\}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m5" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>0.0</mn><mo>,</mo><mn>0.01</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\beta=\{0.0,0.01\}</annotation></semantics></math>, obtaining rewards greater than <math alttext="0.85" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m6" intent=":literal"><semantics><mn>0.85</mn><annotation encoding="application/x-tex">0.85</annotation></semantics></math>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F3.sf2" title="In Figure 3 ‣ A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">3(b)</span></a> further displays the GRPO Pareto front (black line, bottom) across these learning rates, comparing it with the ES Pareto front (blue line, top). The majority of Pareto optimal models across these learning rates obtain a mean reward of less than <math alttext="0.8" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m7" intent=":literal"><semantics><mn>0.8</mn><annotation encoding="application/x-tex">0.8</annotation></semantics></math> and a KL divergence of less than <math alttext="0.4" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m8" intent=":literal"><semantics><mn>0.4</mn><annotation encoding="application/x-tex">0.4</annotation></semantics></math>. The ES Pareto front dominates that of GRPO over different learning rates and <math alttext="\beta" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m9" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> values.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px1.p2">
<p class="ltx_p">Next, the reward distribution for each <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m1" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m2" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> value for GRPO was compared with that of ES, starting with learning rates <math alttext="2\times 10^{-6}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m3" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-6}</annotation></semantics></math> and <math alttext="3\times 10^{-6}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m4" intent=":literal"><semantics><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">3\times 10^{-6}</annotation></semantics></math>. Figures <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F4.sf1" title="In Figure 4 ‣ GRPO with different learning rates. ‣ A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">4(a)</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F4.sf2" title="In Figure 4 ‣ GRPO with different learning rates. ‣ A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">4(b)</span></a> show that all GRPO models stay close to the Qwen2.5-7B-Instruct base model reward distribution, despite the variation in <math alttext="\beta" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m5" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>. In contrast, ES shifts the reward distribution to the right with a density peak around <math alttext="1.0" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m6" intent=":literal"><semantics><mn>1.0</mn><annotation encoding="application/x-tex">1.0</annotation></semantics></math>, i.e. towards higher rewards.
The learning rate was then further increased to <math alttext="4\times 10^{-6}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m7" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">4\times 10^{-6}</annotation></semantics></math> (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F4.sf3" title="In Figure 4 ‣ GRPO with different learning rates. ‣ A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">4(c)</span></a>). As a result, for <math alttext="\beta=0.0" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m8" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0.0</mn></mrow><annotation encoding="application/x-tex">\beta=0.0</annotation></semantics></math> and <math alttext="\beta=0.01" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m9" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">\beta=0.01</annotation></semantics></math>, GRPO shifts the reward distribution to the right towards higher rewards. However, they are still lower than those of ES. As the learning rate is increased further to <math alttext="5\times 10^{-6}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m10" intent=":literal"><semantics><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\times 10^{-6}</annotation></semantics></math> (Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F4.sf4" title="In Figure 4 ‣ GRPO with different learning rates. ‣ A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">4(d)</span></a>), GRPO is sufficiently able to optimize the reward: with <math alttext="\beta=0.0" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m11" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0.0</mn></mrow><annotation encoding="application/x-tex">\beta=0.0</annotation></semantics></math> and <math alttext="\beta=0.01" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m12" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">\beta=0.01</annotation></semantics></math>, it peaks around <math alttext="1.0" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m13" intent=":literal"><semantics><mn>1.0</mn><annotation encoding="application/x-tex">1.0</annotation></semantics></math>. Thus, high learning rate combined with low <math alttext="\beta" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p2.m14" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> is important for GRPO to optimize the reward. However, as was discussed before, such a setting often breaks the performance of the model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A1.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="395" id="A1.F4.sf1.g1" src="./es_files/x4.png" width="664">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Reward distribution with <math alttext="\alpha=2\times 10^{-6}" class="ltx_Math" display="inline" id="A1.F4.sf1.m2" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\alpha=2\times 10^{-6}</annotation></semantics></math>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="395" id="A1.F4.sf2.g1" src="./es_files/x5.png" width="664">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Reward distribution with <math alttext="\alpha=3\times 10^{-6}" class="ltx_Math" display="inline" id="A1.F4.sf2.m2" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\alpha=3\times 10^{-6}</annotation></semantics></math>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="395" id="A1.F4.sf3.g1" src="./es_files/x6.png" width="664">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Reward distribution with <math alttext="\alpha=4\times 10^{-6}" class="ltx_Math" display="inline" id="A1.F4.sf3.m2" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mrow><mn>4</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\alpha=4\times 10^{-6}</annotation></semantics></math>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="401" id="A1.F4.sf4.g1" src="./es_files/x7.png" width="664">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Reward distribution with <math alttext="\alpha=5\times 10^{-6}" class="ltx_Math" display="inline" id="A1.F4.sf4.m2" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\alpha=5\times 10^{-6}</annotation></semantics></math>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Reward distributions in fine-tuning for conciseness with different learning rates <math alttext="\alpha=\{2\times 10^{-6}" class="ltx_math_unparsed" display="inline" id="A1.F4.m6" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\alpha=\{2\times 10^{-6}</annotation></semantics></math>, <math alttext="3\times 10^{-6}" class="ltx_Math" display="inline" id="A1.F4.m7" intent=":literal"><semantics><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">3\times 10^{-6}</annotation></semantics></math>, <math alttext="4\times 10^{-6},5\times 10^{-6}\}" class="ltx_math_unparsed" display="inline" id="A1.F4.m8" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup><mo>,</mo><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">4\times 10^{-6},5\times 10^{-6}\}</annotation></semantics></math> and <math alttext="\beta=\{0.0,0.01,0.1,1.0\}" class="ltx_Math" display="inline" id="A1.F4.m9" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>0.0</mn><mo>,</mo><mn>0.01</mn><mo>,</mo><mn>0.1</mn><mo>,</mo><mn>1.0</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\beta=\{0.0,0.01,0.1,1.0\}</annotation></semantics></math> compared to ES on the Qwen2.5-7B-Instruct base model. Whereas GRPO distribution is similar to the base model, ES shifts it to the right, i.e. higher rewards. Higher rewards can only be achieved with GRPO with high learning rates and low <math alttext="\beta" class="ltx_Math" display="inline" id="A1.F4.m10" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, which setting often breaks to model’s performance.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="250" id="A1.F5.g1" src="./es_files/x8.png" width="664">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Accuracy Improvement over Base Models with ES vs RL across Model Families. ES results in consistently largest improvements in all cases.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Training Curves and Accuracy Improvement of ES and RL on the Countdown Task</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS4.p1">
<p class="ltx_p">As shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F6" title="Figure 6 ‣ A.4 Training Curves and Accuracy Improvement of ES and RL on the Countdown Task ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">6</span></a>, ES consistently outperformed RL across all tested models throughout training. On smaller models such as Qwen2.5-0.5B and Llama-3.2-1B, RL showed almost no improvement, whereas ES steadily increased accuracy. On mid-sized models like Qwen2.5-1.5B, Qwen2.5-3B and Llama-3.2-3B, ES achieved substantial gains, reaching accuracy levels that RL never approached even in extended training. On larger models such as Qwen2.5-7B and Llama-3.1-8B, RL improved more than in the smaller models, but ES still maintained a clear and consistent advantage, achieving the highest accuracy throughout. In addition, as shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F5" title="Figure 5 ‣ GRPO with different learning rates. ‣ A.3 Extended Conciseness Experiments ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">5</span></a>, we compute the relative improvements of PPO, GRPO, and ES over their respective base models across different model families. ES delivers the consistently largest improvements in all cases.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="580" id="A1.F6.g1" src="./es_files/x9.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Training curves of ES and RL across two model families and six sizes in the countdown task. ES fine-tuning results in significantly better performance in all cases. It is able to improve even the smallest model where RL methods are ineffective. ES is also more sample efficient than RL: in most cases, it only needs less than <math alttext="20\%" class="ltx_Math" display="inline" id="A1.F6.m2" intent=":literal"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">20\%</annotation></semantics></math> of the training sample evaluations of RL to achieve similar performance.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Parameter Magnitude Shifts by Evolutionary fine-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS5.p1">
<p class="ltx_p">This section characterizes how parameter magnitudes changed in ES fine-tuning in the countdown and conciseness experiments. Specifically, Figures&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F7" title="Figure 7 ‣ A.5 Parameter Magnitude Shifts by Evolutionary fine-tuning ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">7</span></a> and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F8" title="Figure 8 ‣ A.5 Parameter Magnitude Shifts by Evolutionary fine-tuning ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">8</span></a>, left column, show histograms of the absolute parameter magnitude shifts <math alttext="\Delta" class="ltx_Math" display="inline" id="A1.SS5.p1.m1" intent=":literal"><semantics><mi mathvariant="normal">Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math> before and after finetuning Llama and Qwen models, overlaid with random walk, on the Countdown task reported in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#S4.T1" title="Table 1 ‣ Countdown task. ‣ 4.1 Performance in the Countdown Task ‣ 4 Empirical Studies ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">1</span></a>. The right column in these figures shows the difference between <math alttext="\Delta" class="ltx_Math" display="inline" id="A1.SS5.p1.m2" intent=":literal"><semantics><mi mathvariant="normal">Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math> and the random walk.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS5.p2">
<p class="ltx_p">For most models, <math alttext="\Delta" class="ltx_Math" display="inline" id="A1.SS5.p2.m1" intent=":literal"><semantics><mi mathvariant="normal">Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math> deviates very little from random walk. This is a counterintuitive result since fine-tuning actually resulted in a significant performance boost. A closer inspection reveals that most of the deviation was concentrated around zero. A likely explanation is that there are precision issues around zero, particularly with small bin sizes, which may lead to such deviations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS5.p3">
<p class="ltx_p">More significantly, a systematic deviation from the random walk was observed in conciseness fine-tuning of the largest model, Qwen2.5-7B-Instruct (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F9" title="Figure 9 ‣ A.5 Parameter Magnitude Shifts by Evolutionary fine-tuning ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">9</span></a>). The distribution shifts toward abundant small magnitude edits, suggesting that small parameter tweaks may be most significant in influencing output behavior. This result reinforces observations in prior studies&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(e.g. Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#bib.bib33" title="">2025</a>)</cite>. A possible explanation is that large models encode functionality in a more redundandant manner, and therefore minor tweaks are sufficient to achieve fine-tuning objectives. In fact, the changes are nearly indistinguishable from random walk in Figures&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F7" title="Figure 7 ‣ A.5 Parameter Magnitude Shifts by Evolutionary fine-tuning ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">7</span></a> and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2509.24372v1#A1.F8" title="Figure 8 ‣ A.5 Parameter Magnitude Shifts by Evolutionary fine-tuning ‣ Appendix A Appendix ‣ Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning"><span class="ltx_text ltx_ref_tag">8</span></a> likely because they are benevolent wrt. the fine-tuning objective. A more thorough investigation of these hypotheses is a most interesting direction of future work, potentially resulting in a better understanding of fine-tuning and information processing principles in LLMs in general.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="660" id="A1.F7.g1" src="./es_files/x10.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Parameter magnitude shift histograms for the Countdown task in Llama models optimized by ES. The changes are similar to those of a random walk, concentrated around zero, likely due to numerical inaccuracies.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="933" id="A1.F8.g1" src="./es_files/x11.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Parameter magnitude shift histograms for the Countdown task in Qwen models optimized by ES. The results are consistent with those observed in Llama models.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="542" id="A1.F9.g1" src="./es_files/x12.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Parameter magnitude shift histograms in conciseness fine-tuning in Qwen2.5-7B-Instruct model with ES. In this case, the model is large and the fine-tuning goals is different, revealing a potentially significant pattern of primarily small changes. The hypothesis (to be analyzed more thoroughly in future work) is that behavior is coded in large models in a redundant manner, making it possible to achieve this fine-tuning objective through numerous small changes. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed; display: none; left: 1055.46px; top: 562.646px; transform: translate(-50%, -100%);">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body></html>